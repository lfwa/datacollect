{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Datadynamics","text":"<p>Datadynamics is a Python library and environment for simulating (multi-agent) data collection dynamics. The library is built on top of PettingZoo and is distributed under the BSD 3-Clause License.</p> <p>The documentation is available at lfwa.github.io/datadynamics/.</p>"},{"location":"#installation","title":"Installation","text":"<p>Datadynamics can be installed via PyPI from Python 3.10 and higher:</p> <pre><code>$ pip install datadynamics\n</code></pre> <p>Alternatively, you can install from source by downloading the latest release or by cloning the GitHub repository, navigating into the directory, and installing via Poetry: <code>poetry install</code>.</p>"},{"location":"#usage","title":"Usage","text":"<p>Visit the documentation site at lfwa.github.io/datadynamics/ for full usage guides.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># See tutorials/collector/wrapper_example.py\nfrom datadynamics.environments import collector_v0\nfrom datadynamics.policies import greedy_policy_v0\nenv = collector_v0.env(\nn_points=300, n_agents=2, max_collect=[120, 180], render_mode=\"human\"\n)\npolicy = greedy_policy_v0.policy(env=env)\nenv.reset()\nfor agent in env.agent_iter():\nobservation, reward, termination, truncation, info = env.last()\naction = policy.action(observation, agent)\nenv.step(action)\n</code></pre>"},{"location":"LICENSE/","title":"LICENSE","text":"<p>BSD 3-Clause License</p> <p>Copyright (c) 2022, lfwa</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright notice, this    list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,    this list of conditions and the following disclaimer in the documentation    and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its    contributors may be used to endorse or promote products derived from    this software without specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"api/environments/","title":"Simulation Environments","text":""},{"location":"api/environments/#collector-2d","title":"Collector (2D)","text":""},{"location":"api/environments/#datadynamics.environments.collector.collector.SamplingWrapperEnv","title":"<code>SamplingWrapperEnv</code>","text":"<p>         Bases: <code>raw_env</code></p> <p>Wrapper that creates point and agent positions from a sampler.</p> Source code in <code>datadynamics/environments/collector/collector.py</code> <pre><code>class SamplingWrapperEnv(raw_env):\n\"\"\"Wrapper that creates point and agent positions from a sampler.\n    Attributes:\n        See AECEnv.\n    \"\"\"\ndef __init__(\nself,\nn_agents,\nmax_collect,\nn_points,\nsampler=lambda rng, n: rng.multivariate_normal(\nnp.array([0, 0]), np.array([[1, 0], [0, 1]]), n\n),\n**kwargs,\n):\n\"\"\"Initialize wrapper environment.\n        Args:\n            n_agents (int): Number of agents to create.\n            max_collect (list[int]): List of maximum number of points to\n                collect for each agent.\n            n_points (int): Number of points to generate.\n            sampler (lambda, optional): Lambda function from which n points\n                are generated. Should accept an argument `rng` representing a\n                random generator (rng, e.g. numpy default_rng) and `n`\n                representing number of points to generate. Defaults to a 2D\n                standard Normal distribution.\n        \"\"\"\nsuper().seed()\nassert n_agents &gt; 0, \"n_agents must be greater than 0\"\nassert n_points &gt; 0, \"n_points must be greater than 0\"\nself.n_agents = n_agents\nself.n_points = n_points\nself.sampler = sampler\npoint_positions = self.sampler(self.rng, self.n_points)\npoint_mean = np.mean(point_positions, axis=0)\ninit_agent_positions = np.array(\n[point_mean for _ in range(self.n_agents)]\n)\nsuper().__init__(\npoint_positions=point_positions,\ninit_agent_positions=init_agent_positions,\nmax_collect=max_collect,\n**kwargs,\n)\ndef reset(self, seed=None, return_info=False, options=None):\n\"\"\"Resets the environment to a starting state.\n        Args:\n            seed (int, optional): Random seed to use for resetting. Defaults\n                to None.\n            return_info (bool, optional): Whether to return infos. Defaults to\n                False.\n            options (dict, optional): Additional options. Defaults to None.\n        Returns:\n            dict: Dictionary of observations for each agent. Infos are\n                returned if `return_info` is True.\n        \"\"\"\nreturn super().reset(\nseed=seed, return_info=return_info, options=options\n)\n</code></pre>"},{"location":"api/environments/#datadynamics.environments.collector.collector.SamplingWrapperEnv.__init__","title":"<code>__init__(n_agents, max_collect, n_points, sampler=lambda rng, n: rng.multivariate_normal(np.array([0, 0]), np.array([[1, 0], [0, 1]]), n), **kwargs)</code>","text":"<p>Initialize wrapper environment.</p> <p>Parameters:</p> Name Type Description Default <code>n_agents</code> <code>int</code> <p>Number of agents to create.</p> required <code>max_collect</code> <code>list[int]</code> <p>List of maximum number of points to collect for each agent.</p> required <code>n_points</code> <code>int</code> <p>Number of points to generate.</p> required <code>sampler</code> <code>lambda</code> <p>Lambda function from which n points are generated. Should accept an argument <code>rng</code> representing a random generator (rng, e.g. numpy default_rng) and <code>n</code> representing number of points to generate. Defaults to a 2D standard Normal distribution.</p> <code>lambda rng, n: rng.multivariate_normal(np.array([0, 0]), np.array([[1, 0], [0, 1]]), n)</code> Source code in <code>datadynamics/environments/collector/collector.py</code> <pre><code>def __init__(\nself,\nn_agents,\nmax_collect,\nn_points,\nsampler=lambda rng, n: rng.multivariate_normal(\nnp.array([0, 0]), np.array([[1, 0], [0, 1]]), n\n),\n**kwargs,\n):\n\"\"\"Initialize wrapper environment.\n    Args:\n        n_agents (int): Number of agents to create.\n        max_collect (list[int]): List of maximum number of points to\n            collect for each agent.\n        n_points (int): Number of points to generate.\n        sampler (lambda, optional): Lambda function from which n points\n            are generated. Should accept an argument `rng` representing a\n            random generator (rng, e.g. numpy default_rng) and `n`\n            representing number of points to generate. Defaults to a 2D\n            standard Normal distribution.\n    \"\"\"\nsuper().seed()\nassert n_agents &gt; 0, \"n_agents must be greater than 0\"\nassert n_points &gt; 0, \"n_points must be greater than 0\"\nself.n_agents = n_agents\nself.n_points = n_points\nself.sampler = sampler\npoint_positions = self.sampler(self.rng, self.n_points)\npoint_mean = np.mean(point_positions, axis=0)\ninit_agent_positions = np.array(\n[point_mean for _ in range(self.n_agents)]\n)\nsuper().__init__(\npoint_positions=point_positions,\ninit_agent_positions=init_agent_positions,\nmax_collect=max_collect,\n**kwargs,\n)\n</code></pre>"},{"location":"api/environments/#datadynamics.environments.collector.collector.SamplingWrapperEnv.reset","title":"<code>reset(seed=None, return_info=False, options=None)</code>","text":"<p>Resets the environment to a starting state.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>Random seed to use for resetting. Defaults to None.</p> <code>None</code> <code>return_info</code> <code>bool</code> <p>Whether to return infos. Defaults to False.</p> <code>False</code> <code>options</code> <code>dict</code> <p>Additional options. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary of observations for each agent. Infos are returned if <code>return_info</code> is True.</p> Source code in <code>datadynamics/environments/collector/collector.py</code> <pre><code>def reset(self, seed=None, return_info=False, options=None):\n\"\"\"Resets the environment to a starting state.\n    Args:\n        seed (int, optional): Random seed to use for resetting. Defaults\n            to None.\n        return_info (bool, optional): Whether to return infos. Defaults to\n            False.\n        options (dict, optional): Additional options. Defaults to None.\n    Returns:\n        dict: Dictionary of observations for each agent. Infos are\n            returned if `return_info` is True.\n    \"\"\"\nreturn super().reset(\nseed=seed, return_info=return_info, options=options\n)\n</code></pre>"},{"location":"api/environments/#datadynamics.environments.collector.collector.raw_env","title":"<code>raw_env</code>","text":"<p>         Bases: <code>AECEnv</code></p> <p>Raw collector environment.</p> <p>This environment is based on a 2D plane. Points are given by their (x, y) coordinates and each agent defines a <code>collector</code> that can move around the plane to collect points for a reward. Collectors may only move between points and they always collect the point that they move to. The cost of moving is defined as the Euclidean distance traveled. Agents may also cheat and collect an already collected point. Cheating as well as rewards are defined by a user-given function.</p> Source code in <code>datadynamics/environments/collector/collector.py</code> <pre><code>class raw_env(AECEnv):\n\"\"\"Raw collector environment.\n    This environment is based on a 2D plane. Points are given by their (x, y)\n    coordinates and each agent defines a `collector` that can move around the\n    plane to collect points for a reward. Collectors may only move between\n    points and they always collect the point that they move to.\n    The cost of moving is defined as the Euclidean distance traveled.\n    Agents may also cheat and collect an already collected point. Cheating as\n    well as rewards are defined by a user-given function.\n    Attributes:\n        See AECEnv.\n    \"\"\"\nmetadata = {\n\"name\": \"collector\",\n\"render_modes\": [\"rgb_array\", \"human\"],\n\"is_parrallelizable\": False,\n\"render_fps\": FPS,\n}\ndef __init__(\nself,\npoint_positions,\ninit_agent_positions,\nmax_collect,\ncheating_cost=lambda point: 500 * 0.5,\ncollection_reward=lambda point: 100,\nreveal_cheating_cost=True,\nreveal_collection_reward=True,\nseed=None,\nrender_mode=None,\n):\n\"\"\"Initialize environment.\n        Args:\n            point_positions (np.ndarray): Positions of collectable points as a\n                numpy array with shape (n, 2) representing n (x, y)\n                coordinates.\n            init_agent_positions (np.ndarray): Initial positions of n agents\n                as a numpy array with shape (n, 2) representing n (x, y)\n                coordinates.\n            max_collect (list): List of maximum number of points that each\n                agent can collect. Index i corresponds to agent i given by\n                init_agent_positions.\n            cheating_cost (function, optional): Function that takes a point\n                and returns the cost of cheating by collecting that point.\n                Defaults to lambda point: 500 * 0.5.\n            collection_reward (function, optional): Function that takes a\n                point and returns the reward of collecting that point.\n                Defaults to lambda point: 100.\n            reveal_cheating_cost (bool, optional): Whether to reveal the\n                cheating costs to the agent in observations. Defaults to True.\n            reveal_collection_reward (bool, optional): Whether to reveal the\n                collection rewards to the agent in observations. Defaults to\n                True.\n            seed (int, optional): Seed for random number generator. Defaults\n                to None.\n            render_mode (str, optional): Render mode. Supported modes are\n                specified in environment's metadata[\"render_modes\"] dict.\n                Defaults to None.\n        \"\"\"\nassert (\nrender_mode in self.metadata[\"render_modes\"] or render_mode is None\n), (\nf\"render_mode: {render_mode} is not supported. \"\nf\"Supported modes: {self.metadata['render_modes']}\"\n)\nself.seed(seed)\nself.point_positions = point_positions\nself.agent_positions = init_agent_positions\nself.render_mode = render_mode\nself.cheating_cost = cheating_cost\nself.collection_reward = collection_reward\nself.reveal_cheating_cost = reveal_cheating_cost\nself.reveal_collection_reward = reveal_collection_reward\nself.reward_range = (-np.inf, 0)\nself.agents = [f\"agent_{i}\" for i in range(len(self.agent_positions))]\nself.possible_agents = self.agents[:]\nself.agent_name_mapping = {\nagent: i for i, agent in enumerate(self.agents)\n}\nself._agent_selector = agent_selector(self.agents)\nself.max_collect = {\nagent: max_collect[i] for i, agent in enumerate(self.agents)\n}\nself.scaling, self.translation = self._get_scaling_translation(\nself.point_positions,\nself.agent_positions,\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\nself.action_spaces = self._get_action_spaces(\nself.agents, len(self.point_positions)\n)\nself.observation_spaces = self._get_observation_spaces(\nself.agents,\nself.agent_positions,\nself.point_positions,\nself.reveal_cheating_cost,\nself.reveal_collection_reward,\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\nself.state_space = self._get_state_space(\nself.agent_positions,\nself.point_positions,\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\n# The following are set in reset().\nself.iteration = 0\nself.total_points_collected = 0\nself.points = None\nself.agent_selection = None\nself.has_reset = False\nself.terminate = False\nself.truncate = False\n# Dicts with agent as key.\nself.rewards = None\nself._cumulative_rewards = None\nself.terminations = None\nself.truncations = None\nself.infos = None\nself.collectors = None\nself.cumulative_rewards = None\n# pygame\nself.screen = None\nself.clock = None\nself.surf = None\nself.isopen = False\ndef _get_boundary_arrays(self, array_2d, shape):\n\"\"\"Creates arrays with minimum and maximum with same shape as input.\n        Args:\n            array_2d (np.ndarray): Input array to find minimum and maximum.\n            shape (_type_): Tuple with shape of output arrays.\n        Returns:\n            np.ndarray: Boundary arrays with minimum and maximum.\n        \"\"\"\nboundary_low = np.full(\nshape, np.min(array_2d, axis=0), dtype=np.float64\n)\nboundary_high = np.full(\nshape, np.max(array_2d, axis=0), dtype=np.float64\n)\nreturn boundary_low, boundary_high\ndef _get_obs_state_space(\nself,\nagent_positions,\npoint_positions,\nreveal_cheating_cost,\nreveal_collection_reward,\nscreen_width,\nscreen_height,\n):\n\"\"\"Retrieves global observation/state space.\n        Args:\n            agent_positions (np.ndarray): Agent positions.\n            point_positions (np.ndarray): Point positions.\n            reveal_cheating_cost (bool): Whether to include cheating cost.\n            reveal_collection_reward (bool): Whether to include collection\n                rewards.\n            screen_width (int): Width of display screen.\n            screen_height (int): Height of display screen.\n        Returns:\n            gymnasium.spaces.Dict: Dict space with global observation/state.\n        \"\"\"\nn_points = point_positions.shape[0]\npoint_boundary_low, point_boundary_high = self._get_boundary_arrays(\npoint_positions, shape=(n_points, 2)\n)\nboundary_low, boundary_high = self._get_boundary_arrays(\nnp.concatenate((agent_positions, point_positions)),\nshape=(len(agent_positions), 2),\n)\nspaces = {\n\"point_positions\": gymnasium.spaces.Box(\nlow=point_boundary_low,\nhigh=point_boundary_high,\ndtype=np.float64,\n),\n\"collected\": gymnasium.spaces.Box(\nlow=0, high=np.inf, shape=(n_points,), dtype=int\n),\n\"collector_positions\": gymnasium.spaces.Box(\nlow=boundary_low, high=boundary_high, dtype=np.float64\n),\n\"image\": gymnasium.spaces.Box(\nlow=0,\nhigh=255,\nshape=(screen_width, screen_height, 3),\ndtype=np.uint8,\n),\n}\nif reveal_cheating_cost:\nspaces[\"cheating_cost\"] = gymnasium.spaces.Box(\nlow=-np.inf, high=np.inf, shape=(n_points,), dtype=np.float64\n)\nif reveal_collection_reward:\nspaces[\"collection_reward\"] = gymnasium.spaces.Box(\nlow=-np.inf, high=np.inf, shape=(n_points,), dtype=np.float64\n)\nspace = gymnasium.spaces.Dict(spaces)\nreturn space\ndef _get_action_spaces(self, agents, n_points):\n\"\"\"Retrieves action spaces for all agents.\n        Each action is a point to collect (by index).\n        Args:\n            agents (list[str]): List of agent names.\n            n_points (int): Number of points.\n        Returns:\n            dict: Dictionary of discrete action spaces.\n        \"\"\"\naction_spaces = {\nagent: gymnasium.spaces.Discrete(n_points) for agent in agents\n}\nreturn action_spaces\ndef _get_observation_spaces(\nself,\nagents,\nagent_positions,\npoint_positions,\nreveal_cheating_cost,\nreveal_collection_reward,\nscreen_width,\nscreen_height,\n):\n\"\"\"Retrieves observation spaces for all agents.\n        Each observation consist of the point positions, points collected,\n        agent (incl. inactive) positions, and an image of the environment.\n        Note:\n            These are identical for all agents.\n        Args:\n            agents (list[str]): List of agent names.\n            agent_positions (np.ndarray): Agent positions.\n            point_positions (np.ndarray): Point positions.\n            reveal_cheating_cost (bool): Whether to include cheating cost.\n            reveal_collection_reward (bool): Whether to include collection\n                rewards.\n            screen_width (int): Width of display screen.\n            screen_height (int): Height of display screen.\n        Returns:\n            dict: Dictionary of observation spaces keyed by agent name.\n        \"\"\"\nobservation_spaces = {\nagent: self._get_obs_state_space(\nagent_positions,\npoint_positions,\nreveal_cheating_cost,\nreveal_collection_reward,\nscreen_width,\nscreen_height,\n)\nfor agent in agents\n}\nreturn observation_spaces\ndef _get_state_space(\nself, agent_positions, point_positions, screen_width, screen_height\n):\n\"\"\"Retrieves state space.\n        Args:\n            agent_positions (np.ndarray): Agent positions.\n            point_positions (np.ndarray): Point positions.\n            screen_width (int): Width of display screen.\n            screen_height (int): Height of display screen.\n        Returns:\n            gymnasium.spaces.Dict: State space.\n        \"\"\"\nstate_space = self._get_obs_state_space(\nagent_positions,\npoint_positions,\nTrue,\nTrue,\nscreen_width,\nscreen_height,\n)\nreturn state_space\ndef _get_scaling_translation(\nself,\npoint_positions,\nagent_positions,\nscreen_width,\nscreen_height,\nrelative_padding=0.1,\n):\n\"\"\"Returns scaling and translation factors for x- and y-axis.\n        Fits data on display while preserving aspect ratio.\n        Args:\n            point_positions (np.ndarray): Point positions.\n            agent_positions (np.ndarray): Agent positions.\n            screen_width (int): Width of display screen.\n            screen_height (int): Height of display screen.\n            relative_padding (float, optional): Outside padding.\n                Defaults to 0.1.\n        Returns:\n            tuple: Tuple of scaling and translation factors for x and y-axis.\n                ((scale_x, scale_y), (translate_x, translate_y)).\n        \"\"\"\nassert 0 &lt;= relative_padding &lt; 1, \"Relative padding must be in [0,1).\"\npos = np.concatenate((point_positions, agent_positions), axis=0)\nminimum = np.min(pos, axis=0)\nmaximum = np.max(pos, axis=0)\nx_min, y_min, x_max, y_max = (\nminimum[0],\nminimum[1],\nmaximum[0],\nmaximum[1],\n)\nx_range = x_max - x_min\ny_range = y_max - y_min\nmax_range = np.max([x_range, y_range, 1])\nupper_x = screen_width * (1 - relative_padding)\nlower_x = screen_width * relative_padding\nscaling_x = (upper_x - lower_x) / max_range\ntranslation_x = -x_min * scaling_x + lower_x\nupper_y = screen_height * (1 - relative_padding)\nlower_y = screen_height * relative_padding\nscaling_y = (upper_y - lower_y) / max_range\ntranslation_y = -y_min * scaling_y + lower_y\nreturn (scaling_x, scaling_y), (translation_x, translation_y)\ndef _scale_position(self, position):\n\"\"\"Scale a position using stored scaling and translation factors.\n        Args:\n            position (tuple): (x, y) tuple of position.\n        Returns:\n            tuple: Scaled (x, y) tuple of position.\n        \"\"\"\nx, y = position\nx = x * self.scaling[0] + self.translation[0]\ny = y * self.scaling[1] + self.translation[1]\nreturn x, y\ndef _create_collectors(\nself, agent_positions, agents, scaling, translation\n):\n\"\"\"Creates collector for each agent as a dict.\n        Args:\n            agent_positions (np.ndarray): Agent positions.\n            agents (list[str]): List of agent names.\n            scaling (tuple): Scaling factors for displaying.\n            translation (tuple): Translation factors for displaying.\n        Returns:\n            dict: Dictionary of collectors keyed by agent name.\n        \"\"\"\ncollectors = {\nagent: Collector(\npos=position,\nscaling=scaling,\ntranslation=translation,\nid=f\"collector_{agent}\",\n)\nfor agent, position in zip(agents, agent_positions)\n}\nreturn collectors\ndef _create_points(self, point_positions, scaling, translation):\n\"\"\"Creates points for all given positions.\n        Args:\n            point_positions (np.ndarray): Point positions.\n            scaling (tuple): Scaling factors for displaying.\n            translation (tuple): Translation factors for displaying.\n        Returns:\n            list[Point]: List of points.\n        \"\"\"\npoints = [\nPoint(\npos=position,\nscaling=scaling,\ntranslation=translation,\nid=f\"point_{i}\",\n)\nfor i, position in enumerate(point_positions)\n]\nreturn points\ndef _create_image_array(self, surf, size):\n\"\"\"Returns image array from pygame surface.\n        Args:\n            surf (pygame.Surface): Surface to convert to image array.\n            size (tuple): Tuple of (width, height) to scale surface to.\n        Returns:\n            np.ndarray: Image array.\n        \"\"\"\nscaled_surf = pygame.transform.smoothscale(surf, size)\nreturn np.transpose(\nnp.array(pygame.surfarray.pixels3d(scaled_surf)), axes=(1, 0, 2)\n)\ndef reward(self, collector, point):\n\"\"\"Returns reward for collecting a given point.\n        Collecting a point triggers a reward, but if the point has already\n        been collected, we add a penalty for cheating. The cost of moving\n        is the Euclidean distance.\n        Args:\n            collector (Collector): Collector that collected the point.\n            point (Point): Point that is collected.\n        Returns:\n            float: Reward.\n        \"\"\"\nreward = -np.linalg.norm(collector.position - point.position)\nreward += self.collection_reward(point)\nif point.is_collected():\nreward -= self.cheating_cost(point)\nreturn reward\ndef _state(\nself,\npoints,\ncollectors,\nreveal_cheating_cost,\nreveal_collection_reward,\n):\n\"\"\"Retrieves state of the current global environment.\n        Args:\n            points (list[Point]): List of points in the environment.\n            collectors (dict): Dictionary of collectors.\n            reveal_cheating_cost (bool): Whether to reveal cheating cost.\n            reveal_collection_reward (bool): Whether to reveal collection\n                reward.\n        Returns:\n            dict: Current global state.\n        \"\"\"\nstate = {\n\"point_positions\": np.array(\n[point.position for point in points], dtype=np.float64\n),\n\"collected\": np.array(\n[point.get_collect_counter() for point in points], dtype=int\n),\n\"collector_positions\": np.array(\n[collector.position for collector in collectors.values()],\ndtype=np.float64,\n),\n\"image\": self._render(render_mode=\"rgb_array\"),\n}\nif reveal_cheating_cost:\nstate[\"cheating_cost\"] = np.array(\n[self.cheating_cost(point) for point in points],\ndtype=np.float64,\n)\nif reveal_collection_reward:\nstate[\"collection_reward\"] = np.array(\n[self.collection_reward(point) for point in points],\ndtype=np.float64,\n)\nreturn state\ndef observe(self, agent):\n# FIXME: Warning for api_test /Users/lfwa/Library/Caches/pypoetry/\n# virtualenvs/collector-gjPrMD7k-py3.10/lib/python3.10/site-packages/\n# pettingzoo/test/api_test.py:60: UserWarning: Observation is not\n# NumPy array\n# warnings.warn(\"Observation is not NumPy array\")\nreturn self._state(\nself.points,\nself.collectors,\nself.reveal_cheating_cost,\nself.reveal_collection_reward,\n)\ndef state(self):\nreturn self._state(self.points, self.collectors, True, True)\ndef reset(self, seed=None, return_info=False, options=None):\nif seed is not None:\nself.seed(seed)\nself.agents = self.possible_agents[:]\nself._agent_selector.reinit(self.agents)\nself.agent_selection = self._agent_selector.reset()\nself.collectors = self._create_collectors(\nself.agent_positions, self.agents, self.scaling, self.translation\n)\nself.points = self._create_points(\nself.point_positions, self.scaling, self.translation\n)\nself.iteration = 0\nself.total_points_collected = 0\nself.has_reset = True\nself.terminate = False\nself.truncate = False\nself.rewards = {agent: 0 for agent in self.agents}\nself._cumulative_rewards = {agent: 0 for agent in self.agents}\nself.terminations = {agent: False for agent in self.agents}\nself.truncations = {agent: False for agent in self.agents}\nself.infos = {agent: {} for agent in self.agents}\nself.cumulative_rewards = {agent: 0 for agent in self.agents}\nobservations = {agent: self.observe(agent) for agent in self.agents}\nif not return_info:\nreturn observations\nelse:\nreturn observations, self.infos\ndef step(self, action):\nassert (\nself.has_reset\n), \"Environment has not been reset yet. Call env.reset() first.\"\nagent = self.agent_selection\nif self.terminations[agent] or self.truncations[agent]:\nself._was_dead_step(action)\n# Guard against first agent dying first since _was_dead_step()\n# does not update agent_selection when that happens.\nif self.agent_selection not in self.agents and self.agents:\nself.agent_selection = self._agent_selector.next()\nreturn\nif (\nnot self.action_space(agent).contains(action)\nand action is not None\n):\nraise ValueError(f\"Action {action} is invalid for agent {agent}.\")\nif action is not None:\npoint_to_collect = self.points[action]\ncollector = self.collectors[agent]\nreward = self.reward(collector, point_to_collect)\n# Move collector to point position.\ncollector.move(point_to_collect.position)\n# Only collect point after reward has been calculated.\ncollector.collect(point_to_collect, self.total_points_collected)\nself.total_points_collected += 1\nelse:\nreward = 0\n# Update termination and truncation for agent.\nif (\nself.collectors[agent].total_points_collected\n&gt;= self.max_collect[agent]\n):\nself.terminations[agent] = True\nself.terminate = all(self.terminations.values())\nself.truncate = all(self.truncations.values())\nself.iteration += 1\nself.rewards[agent] = reward\nself.cumulative_rewards[agent] += reward\n# Cumulative reward since agent has last acted.\nself._cumulative_rewards[agent] = 0\nself._accumulate_rewards()\nself.agent_selection = self._agent_selector.next()\nif self.render_mode == \"human\":\nself.render()\ndef render(self):\nassert (\nself.has_reset\n), \"Environment has not been reset yet. Call env.reset() first.\"\nif self.render_mode is None:\ngymnasium.logger.warn(\nf\"No render mode specified, skipping render. Please \"\n\"specify render_mode as one of the supported modes \"\nf\"{self.metadata['render_modes']} at initialization.\"\n)\nelse:\nreturn self._render(render_mode=self.render_mode)\ndef _render(self, render_mode):\n\"\"\"Renders the environment.\n        Args:\n            render_mode (str): One of the supported render modes.\n        Returns:\n            np.ndarray or None: Returns the rendered image if render_mode is\n                `rgb_array`, otherwise returns None.\n        \"\"\"\npygame.font.init()\nif self.screen is None and render_mode == \"human\":\npygame.init()\npygame.display.init()\nself.screen = pygame.display.set_mode(\n(SCREEN_WIDTH, SCREEN_HEIGHT)\n)\nif self.clock is None:\nself.clock = pygame.time.Clock()\nself.surf = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT))\n# Add white background.\nself.surf.fill((255, 255, 255))\nself._render_points(self.surf, self.points, POINT_SIZE)\nself._render_paths(self.surf, self.collectors, PATH_SIZE)\nself._render_collectors(\nself.surf, self.collectors, COLLECTOR_LEN, COLLECTOR_SIZE\n)\n# Flip y-axis since pygame has origin at top left.\nself.surf = pygame.transform.flip(self.surf, flip_x=False, flip_y=True)\nself._render_text(self.surf)\nif render_mode == \"human\":\npygame.event.pump()\nself.clock.tick(self.metadata[\"render_fps\"])\nassert self.screen is not None\nself.screen.blit(self.surf, (0, 0))\npygame.display.update()\nelif render_mode == \"rgb_array\":\nreturn self._create_image_array(\nself.surf, (SCREEN_WIDTH, SCREEN_HEIGHT)\n)\ndef _render_text(self, surf):\n\"\"\"Renders information text, e.g. stats about environment and actions.\n        Args:\n            surf (pygame.Surface): Surface to render text on.\n        \"\"\"\n(\nstats,\noverall_total_points_collected,\noverall_unique_points_collected,\noverall_cheated,\n) = self._get_stats()\ntotal_reward = sum(self.cumulative_rewards.values())\nfont = pygame.font.Font(pygame.font.get_default_font(), FONT_SIZE)\ntext1 = font.render(\n(\nf\"Iteration: {self.iteration} | \"\nf\"Total points collected: {overall_total_points_collected} | \"\n\"Unique points collected: \"\nf\"{overall_unique_points_collected} / {len(self.points)} | \"\nf\"Cheated: {overall_cheated}\"\n),\nTrue,\n(0, 0, 255),\n)\ntext2 = font.render(\nf\"Total cumulative reward: {total_reward}\",\nTrue,\n(0, 0, 255),\n)\nsurf.blit(text1, (10, 10))\nsurf.blit(text2, (10, 40))\ndef _get_stats(self):\n\"\"\"Retrieves stats for all collectors.\n        Returns:\n            tuple: Tuple of stats.\n        \"\"\"\nstats = {}\noverall_total_points_collected = 0\noverall_unique_points_collected = 0\noverall_cheated = 0\nfor agent in self.collectors:\ncollector = self.collectors[agent]\nstats[agent] = {\n\"total_points_collected\": collector.total_points_collected,\n\"unique_points_collected\": collector.unique_points_collected,\n\"cheated\": collector.cheated,\n}\noverall_total_points_collected += collector.total_points_collected\noverall_unique_points_collected += (\ncollector.unique_points_collected\n)\noverall_cheated += collector.cheated\nreturn (\nstats,\noverall_total_points_collected,\noverall_unique_points_collected,\noverall_cheated,\n)\ndef _render_points(self, surf, points, point_size):\n\"\"\"Renders all points as circles.\n        Points are colored according to their collector as a pie chart.\n        Args:\n            surf (pygame.Surface): Surface to render points on.\n            points (list[Points]): List of points to render.\n            point_size (int): Render size of points.\n        \"\"\"\nfor point in points:\nx, y = tuple(point.scaled_position - (point_size / 2))\nbounding_box = pygame.Rect(x, y, point_size, point_size)\ntotal_collections = point.get_collect_counter()\nstart_angle = 0\nif total_collections == 0:\npygame.draw.circle(\nsurf,\npoint.color,\ntuple(point.scaled_position),\npoint_size / 2,\n)\nelse:\nfor (\ncollector_id,\ncollections,\n) in point.collector_tracker.items():\nif collections == 0:\ncontinue\narc_length = collections / total_collections * 2 * math.pi\npygame.draw.arc(\nsurf,\nself.collectors[collector_id[10:]].color,\nbounding_box,\nstart_angle,\nstart_angle + arc_length,\npoint_size,\n)\nstart_angle += arc_length\ndef _render_paths(self, surf, collectors, path_size):\n\"\"\"Renders paths taken by collectors.\n        Colors are assigned to paths based on the collector that took it.\n        If paths overlap then they are colored in segments.\n        Args:\n            surf (pygame.Surface): Surface to render paths on.\n            collectors (dict): Dict of collectors.\n            path_size (int): Render size of paths.\n        \"\"\"\npath_pairs = {}\nfor collector in collectors.values():\npath_pos_len = len(collector.path_positions)\nif path_pos_len &lt; 2:\ncontinue\nfor i in range(1, path_pos_len):\nkey = (\ntuple(collector.path_positions[i - 1]),\ntuple(collector.path_positions[i]),\n)\nreverse_key = (key[1], key[0])\n# We should not care whether it is (a, b) or (b, a).\nif key in path_pairs:\npath_pairs[key] += [collector]\nelif reverse_key in path_pairs:\npath_pairs[reverse_key] += [collector]\nelse:\npath_pairs[key] = [collector]\nfor path, collectors in path_pairs.items():\ntotal_collectors = len(collectors)\nprev_x, prev_y = self._scale_position(path[0])\nx, y = self._scale_position(path[1])\nsegment_x = (x - prev_x) / total_collectors\nsegment_y = (y - prev_y) / total_collectors\nfor collector in collectors:\npygame.draw.line(\nsurf,\ncollector.color,\n(prev_x, prev_y),\n(prev_x + segment_x, prev_y + segment_y),\npath_size,\n)\nprev_x += segment_x\nprev_y += segment_y\ndef _render_collectors(\nself, surf, collectors, collector_len, collector_size\n):\n\"\"\"Renders all collectors as crosses.\n        Collectors are rotated when stacked to avoid overlapping.\n        Black borders are added to crosses.\n        Args:\n            surf (pygame.Surface): Surface to render collectors on.\n            collectors (dict): Dict of collectors.\n            collector_len (int): Length of collector cross.\n            collector_size (int): Size of collector cross.\n        \"\"\"\nfor position, colls in groupby(\ncollectors.values(), lambda col: tuple(col.position)\n):\ncolls = list(colls)\nposition = colls[0].scaled_position\ntotal_collectors = len(colls)\nshift_increment = collector_len / total_collectors\nshift = collector_len / 2\nfor i, collector in enumerate(colls):\ncross_rotate_shift = i * shift_increment\n# Add black border to cross.\nborder_size = math.ceil(collector_size * 1.7)\npygame.draw.line(\nsurf,\n(0, 0, 0),\nstart_pos=(\nposition[0] + cross_rotate_shift - shift,\nposition[1] - shift,\n),\nend_pos=(\nposition[0] + shift - cross_rotate_shift,\nposition[1] + shift,\n),\nwidth=border_size,\n)\npygame.draw.line(\nsurf,\n(0, 0, 0),\nstart_pos=(\nposition[0] + shift,\nposition[1] + cross_rotate_shift - shift,\n),\nend_pos=(\nposition[0] - shift,\nposition[1] + shift - cross_rotate_shift,\n),\nwidth=border_size,\n)\n# Draw cross.\npygame.draw.line(\nsurf,\ncollector.color,\nstart_pos=(\nposition[0] + cross_rotate_shift - shift,\nposition[1] - shift,\n),\nend_pos=(\nposition[0] + shift - cross_rotate_shift,\nposition[1] + shift,\n),\nwidth=collector_size,\n)\npygame.draw.line(\nsurf,\ncollector.color,\nstart_pos=(\nposition[0] + shift,\nposition[1] + cross_rotate_shift - shift,\n),\nend_pos=(\nposition[0] - shift,\nposition[1] + shift - cross_rotate_shift,\n),\nwidth=collector_size,\n)\ndef observation_space(self, agent):\nreturn self.observation_spaces[agent]\ndef action_space(self, agent):\nreturn self.action_spaces[agent]\ndef seed(self, seed=None):\nself.rng, seed = gymnasium.utils.seeding.np_random(seed)\nreturn [seed]\ndef close(self):\nif self.screen is not None:\npygame.display.quit()\nself.isopen = False\npygame.quit()\n</code></pre>"},{"location":"api/environments/#datadynamics.environments.collector.collector.raw_env.__init__","title":"<code>__init__(point_positions, init_agent_positions, max_collect, cheating_cost=lambda point: 500 * 0.5, collection_reward=lambda point: 100, reveal_cheating_cost=True, reveal_collection_reward=True, seed=None, render_mode=None)</code>","text":"<p>Initialize environment.</p> <p>Parameters:</p> Name Type Description Default <code>point_positions</code> <code>np.ndarray</code> <p>Positions of collectable points as a numpy array with shape (n, 2) representing n (x, y) coordinates.</p> required <code>init_agent_positions</code> <code>np.ndarray</code> <p>Initial positions of n agents as a numpy array with shape (n, 2) representing n (x, y) coordinates.</p> required <code>max_collect</code> <code>list</code> <p>List of maximum number of points that each agent can collect. Index i corresponds to agent i given by init_agent_positions.</p> required <code>cheating_cost</code> <code>function</code> <p>Function that takes a point and returns the cost of cheating by collecting that point. Defaults to lambda point: 500 * 0.5.</p> <code>lambda point: 500 * 0.5</code> <code>collection_reward</code> <code>function</code> <p>Function that takes a point and returns the reward of collecting that point. Defaults to lambda point: 100.</p> <code>lambda point: 100</code> <code>reveal_cheating_cost</code> <code>bool</code> <p>Whether to reveal the cheating costs to the agent in observations. Defaults to True.</p> <code>True</code> <code>reveal_collection_reward</code> <code>bool</code> <p>Whether to reveal the collection rewards to the agent in observations. Defaults to True.</p> <code>True</code> <code>seed</code> <code>int</code> <p>Seed for random number generator. Defaults to None.</p> <code>None</code> <code>render_mode</code> <code>str</code> <p>Render mode. Supported modes are specified in environment's metadata[\"render_modes\"] dict. Defaults to None.</p> <code>None</code> Source code in <code>datadynamics/environments/collector/collector.py</code> <pre><code>def __init__(\nself,\npoint_positions,\ninit_agent_positions,\nmax_collect,\ncheating_cost=lambda point: 500 * 0.5,\ncollection_reward=lambda point: 100,\nreveal_cheating_cost=True,\nreveal_collection_reward=True,\nseed=None,\nrender_mode=None,\n):\n\"\"\"Initialize environment.\n    Args:\n        point_positions (np.ndarray): Positions of collectable points as a\n            numpy array with shape (n, 2) representing n (x, y)\n            coordinates.\n        init_agent_positions (np.ndarray): Initial positions of n agents\n            as a numpy array with shape (n, 2) representing n (x, y)\n            coordinates.\n        max_collect (list): List of maximum number of points that each\n            agent can collect. Index i corresponds to agent i given by\n            init_agent_positions.\n        cheating_cost (function, optional): Function that takes a point\n            and returns the cost of cheating by collecting that point.\n            Defaults to lambda point: 500 * 0.5.\n        collection_reward (function, optional): Function that takes a\n            point and returns the reward of collecting that point.\n            Defaults to lambda point: 100.\n        reveal_cheating_cost (bool, optional): Whether to reveal the\n            cheating costs to the agent in observations. Defaults to True.\n        reveal_collection_reward (bool, optional): Whether to reveal the\n            collection rewards to the agent in observations. Defaults to\n            True.\n        seed (int, optional): Seed for random number generator. Defaults\n            to None.\n        render_mode (str, optional): Render mode. Supported modes are\n            specified in environment's metadata[\"render_modes\"] dict.\n            Defaults to None.\n    \"\"\"\nassert (\nrender_mode in self.metadata[\"render_modes\"] or render_mode is None\n), (\nf\"render_mode: {render_mode} is not supported. \"\nf\"Supported modes: {self.metadata['render_modes']}\"\n)\nself.seed(seed)\nself.point_positions = point_positions\nself.agent_positions = init_agent_positions\nself.render_mode = render_mode\nself.cheating_cost = cheating_cost\nself.collection_reward = collection_reward\nself.reveal_cheating_cost = reveal_cheating_cost\nself.reveal_collection_reward = reveal_collection_reward\nself.reward_range = (-np.inf, 0)\nself.agents = [f\"agent_{i}\" for i in range(len(self.agent_positions))]\nself.possible_agents = self.agents[:]\nself.agent_name_mapping = {\nagent: i for i, agent in enumerate(self.agents)\n}\nself._agent_selector = agent_selector(self.agents)\nself.max_collect = {\nagent: max_collect[i] for i, agent in enumerate(self.agents)\n}\nself.scaling, self.translation = self._get_scaling_translation(\nself.point_positions,\nself.agent_positions,\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\nself.action_spaces = self._get_action_spaces(\nself.agents, len(self.point_positions)\n)\nself.observation_spaces = self._get_observation_spaces(\nself.agents,\nself.agent_positions,\nself.point_positions,\nself.reveal_cheating_cost,\nself.reveal_collection_reward,\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\nself.state_space = self._get_state_space(\nself.agent_positions,\nself.point_positions,\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\n# The following are set in reset().\nself.iteration = 0\nself.total_points_collected = 0\nself.points = None\nself.agent_selection = None\nself.has_reset = False\nself.terminate = False\nself.truncate = False\n# Dicts with agent as key.\nself.rewards = None\nself._cumulative_rewards = None\nself.terminations = None\nself.truncations = None\nself.infos = None\nself.collectors = None\nself.cumulative_rewards = None\n# pygame\nself.screen = None\nself.clock = None\nself.surf = None\nself.isopen = False\n</code></pre>"},{"location":"api/environments/#datadynamics.environments.collector.collector.raw_env.reward","title":"<code>reward(collector, point)</code>","text":"<p>Returns reward for collecting a given point.</p> <p>Collecting a point triggers a reward, but if the point has already been collected, we add a penalty for cheating. The cost of moving is the Euclidean distance.</p> <p>Parameters:</p> Name Type Description Default <code>collector</code> <code>Collector</code> <p>Collector that collected the point.</p> required <code>point</code> <code>Point</code> <p>Point that is collected.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Reward.</p> Source code in <code>datadynamics/environments/collector/collector.py</code> <pre><code>def reward(self, collector, point):\n\"\"\"Returns reward for collecting a given point.\n    Collecting a point triggers a reward, but if the point has already\n    been collected, we add a penalty for cheating. The cost of moving\n    is the Euclidean distance.\n    Args:\n        collector (Collector): Collector that collected the point.\n        point (Point): Point that is collected.\n    Returns:\n        float: Reward.\n    \"\"\"\nreward = -np.linalg.norm(collector.position - point.position)\nreward += self.collection_reward(point)\nif point.is_collected():\nreward -= self.cheating_cost(point)\nreturn reward\n</code></pre>"},{"location":"api/environments/#datadynamics.environments.collector.collector.env","title":"<code>env(**kwargs)</code>","text":"<p>Creates a collector environment.</p> <p>Returns:</p> Type Description <p>pettingzoo.utils.env.AECEnv: Created environment.</p> Source code in <code>datadynamics/environments/collector/collector.py</code> <pre><code>def env(**kwargs):\n\"\"\"Creates a collector environment.\n    Returns:\n        pettingzoo.utils.env.AECEnv: Created environment.\n    \"\"\"\nif \"n_points\" in kwargs:\nenv = SamplingWrapperEnv(**kwargs)\nelse:\nenv = raw_env(**kwargs)\nreturn env\n</code></pre>"},{"location":"api/environments/#graph-collector","title":"Graph Collector","text":""},{"location":"api/environments/#datadynamics.environments.graph_collector.graph_collector.raw_env","title":"<code>raw_env</code>","text":"<p>         Bases: <code>AECEnv</code></p> <p>Raw graph collector environment.</p> <p>This environment is based on a weighted (possible directed) graph using networkx. The graph represents the environment structure and may define obstacles by creating nodes with e.g. no connecting edges as well as define collectable points. The weight of each edge defines the cost of traversing that edge. Agents can move around the graph by traversing edges and collect defined points for a reward. Agents can also cheat by collecting points that have already been collected. Cheating as well as rewards are defined by a user-given function.</p> Source code in <code>datadynamics/environments/graph_collector/graph_collector.py</code> <pre><code>class raw_env(AECEnv):\n\"\"\"Raw graph collector environment.\n    This environment is based on a weighted (possible directed) graph using\n    networkx. The graph represents the environment structure and may define\n    obstacles by creating nodes with e.g. no connecting edges as well as\n    define collectable points. The weight of each edge defines the cost of\n    traversing that edge.\n    Agents can move around the graph by traversing edges and collect defined\n    points for a reward. Agents can also cheat by collecting points that have\n    already been collected. Cheating as well as rewards are\n    defined by a user-given function.\n    Attributes:\n        See AECEnv.\n    \"\"\"\nmetadata = {\n\"name\": \"graph_collector\",\n\"render_modes\": [\"rgb_array\", \"human\"],\n\"is_parrallelizable\": False,\n\"render_fps\": FPS,\n}\ndef __init__(\nself,\ngraph,\npoint_labels,\ninit_agent_labels,\nmax_collect,\nnodes_per_row=None,\ncheating_cost=lambda point_label: 500 * 0.5,\ncollection_reward=lambda point_label: 100,\nreveal_cheating_cost=True,\nreveal_collection_reward=True,\nstatic_graph=True,\ndynamic_display=False,\nseed=None,\nrender_mode=None,\n):\n\"\"\"Initializes the graph collector environment.\n        Args:\n            graph (networkx.Graph): Input directed or undirected graph\n                defining the environment. Node labels must be a continuous set\n                of integers starting at 0.\n            point_labels (list[int]): List of node labels to identify\n                collectable points. All duplicate labels will be merged when\n                creating points in the environment.\n            init_agent_labels (list[int]): List of node labels to identify\n                initial agent positions.\n            max_collect (list[int]): List of maximum number of points each\n                agent can collect.\n            nodes_per_row (int, optional): Number of nodes to display per row.\n                Defaults to None.\n            cheating_cost (function, optional): Function that takes a node\n                label and returns the cost of cheating by collecting an\n                already collected point. Influences reward for collecting\n                points. Defaults to lambda node_label: 500 * 0.5.\n            collection_reward (function, optional): Function that takes a point\n                label and returns the reward for collecting that point.\n                Defaults to lambda point_label: 100.\n            reveal_cheating_cost (bool, optional): Whether to reveal the\n                cheating costs to the agent in observations. Defaults to True.\n            reveal_collection_reward (bool, optional): Whether to reveal the\n                collection rewards to the agent in observations. Defaults to\n                True.\n            static_graph (bool, optional): Whether the underlying graph is\n                static and never changes. Significantly impacts performance\n                since everything in the environment have to be re-rendered\n                for every frame to ensure consistency. May also influence the\n                performance of policies as e.g. shortest paths will need to be\n                recomputed for every action to determine optimal agent\n                movement. Defaults to True.\n            dynamic_display (bool, optional): Whether to dynamically adjust\n                the display size to the graph size. Defaults to False.\n            seed (int, optional): Seed for random number generator. Defaults\n                to None.\n            render_mode (str, optional): Render mode. Supported modes are\n                specified in environment's metadata[\"render_modes\"] dict.\n                Defaults to None.\n        \"\"\"\ngymnasium.logger.info(\"Initializing graph collector environment...\")\nassert (\nrender_mode in self.metadata[\"render_modes\"] or render_mode is None\n), (\nf\"render_mode: {render_mode} is not supported. \"\nf\"Supported modes: {self.metadata['render_modes']}\"\n)\ngymnasium.logger.info(\" - Validating input positions...\")\n# Check that all agent and point labels are nodes in the graph.\nassert all(\nlabel in graph.nodes for label in init_agent_labels\n), \"Agent labels must be nodes in the graph!\"\nassert all(\nlabel in graph.nodes for label in point_labels\n), \"Point labels must be nodes in the graph!\"\n# Disallow agents to spawn inside obstacles as they would\n# not be able to move, i.e., agent labels must have neighbors.\nassert all(\nany(True for _ in graph.neighbors(agent_label))\nfor agent_label in init_agent_labels\n), \"Agent labels must not encode obstacles!\"\n# Allow but issue warning for points inside obstacles as they just\n# cannot be collected.\nif any(\nnot any(True for _ in graph.neighbors(point_label))\nfor point_label in point_labels\n):\ngymnasium.logger.warn(\n\"Some points are inside obstacles and cannot be collected!\"\n)\n# Warn if points are overlapping.\nif len(point_labels) != len(set(point_labels)):\ngymnasium.logger.warn(\"Some points overlap and will be merged!\")\ngymnasium.logger.info(\" - Seeding environment...\")\nself.seed(seed)\nself.graph = graph\n# Remove duplicate points.\nself._point_labels = list(dict.fromkeys(point_labels))\nself.init_agent_labels = init_agent_labels\nself.render_mode = render_mode\nself.cheating_cost = cheating_cost\nself.collection_reward = collection_reward\nself.reveal_cheating_cost = reveal_cheating_cost\nself.reveal_collection_reward = reveal_collection_reward\nself.static_graph = static_graph\nif nodes_per_row is None:\nnodes_per_row = math.ceil(math.sqrt(len(self.graph.nodes)))\nself.nodes_per_row = nodes_per_row\n# For dynamic displaying, we adjust the screen width s.t.\n# nodes are displayed as squares.\nif dynamic_display:\nglobal SCREEN_WIDTH\nrows = math.ceil(len(self.graph.nodes) / nodes_per_row)\naspect_ratio = self.nodes_per_row / rows\nSCREEN_WIDTH = int(SCREEN_HEIGHT * aspect_ratio)\nself.node_width, self.node_height = self._get_node_shape(\nlen(self.graph.nodes),\nself.nodes_per_row,\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\nself.reward_range = (-np.inf, 0)\nself.agents = [\nf\"agent_{i}\" for i in range(len(self.init_agent_labels))\n]\nself.possible_agents = self.agents[:]\nself.agent_name_mapping = {\nagent: i for i, agent in enumerate(self.agents)\n}\nself._agent_selector = agent_selector(self.agents)\nself.max_collect = {\nagent: max_collect[i] for i, agent in enumerate(self.agents)\n}\ngymnasium.logger.info(\" - Setting up action spaces...\")\nself.action_spaces = self._get_action_spaces(\nself.agents, self.graph.nodes\n)\ngymnasium.logger.info(\" - Setting up observation spaces...\")\nself.observation_spaces = self._get_observation_spaces(\nlen(self.graph.nodes),\nlen(self._point_labels),\nself.agents,\nself.reveal_cheating_cost,\nself.reveal_collection_reward,\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\ngymnasium.logger.info(\" - Setting up state space...\")\nself.state_space = self._get_state_space(\nlen(self.graph.nodes),\nlen(self._point_labels),\nlen(self.agents),\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\n# The following are set in reset().\nself.iteration = 0\nself.total_points_collected = 0\nself.points = None\nself.agent_selection = None\nself.has_reset = False\nself.terminate = False\nself.truncate = False\n# Dicts with agent as key.\nself.rewards = None\nself._cumulative_rewards = None\nself.terminations = None\nself.truncations = None\nself.infos = None\nself.collectors = None\nself.cumulative_rewards = None\n# pygame\nself.screen = None\nself.clock = None\nself.surf = None\nself.cached_obstacle_surf = None\nself.isopen = False\ngymnasium.logger.info(\"Environment initialized.\")\ndef _get_node_shape(\nself, n_nodes, nodes_per_row, screen_width, screen_height\n):\n\"\"\"Returns the display width and height of a node.\n        Args:\n            n_nodes (int): Number of nodes in the graph.\n            nodes_per_row (int): Number of nodes to display per row.\n            screen_width (int): Width of the display.\n            screen_height (int): Height of the display.\n        Returns:\n            tuple: Tuple containing the display width and height of a node.\n        \"\"\"\nwidth = screen_width / nodes_per_row\nheight = screen_height / math.ceil(n_nodes / nodes_per_row)\nreturn width, height\ndef _get_action_spaces(self, agents, nodes):\n\"\"\"Retrieves action spaces for all agents.\n        Each action is a neighbouring node to move to (by node label) or\n        to collect the current point (if the node is defined as one) by\n        issuing the `collect` action, -1.\n        Args:\n            agents (list[str]): List of agent names.\n            nodes (list[int]): List of node labels.\n        Returns:\n            dict: Dictionary of discrete action spaces.\n        \"\"\"\naction_spaces = {\nagent: gymnasium.spaces.Discrete(n=len(nodes) + 1, start=-1)\nfor agent in agents\n}\ndef sample(mask=None):\n\"\"\"Generates a sample from the space.\n            A sample is a neighbouring node chosen uniformly at random.\n            Args:\n                mask (np.ndarray, optional): An optimal mask for if an action\n                    can be selected where `1` represents valid actions and `0`\n                    invalid or infeasible actions. Defaults to None.\n            Returns:\n                int: Node label of the randomly sampled neighbouring node.\n            \"\"\"\nagent = self.agent_selection\nassert agent is not None, (\n\"Agent is required to sample action but none is selected yet. \"\n\"Did you call reset() before sampling?\"\n)\nassert self.collectors is not None, (\n\"Collectors are required to sample action but none are \"\n\"created yet. Did you call reset() before sampling?\"\n)\naction_mask, collect_action_validity = self._get_action_mask(agent)\npossible_actions = action_mask.nonzero()[0]\nif collect_action_validity:\npossible_actions = np.append(possible_actions, -1)\nreturn (\nself.rng.choice(possible_actions)\nif not possible_actions.size == 0\nelse None\n)\n# Replace standard sample method s.t. we check for path validity.\nfor action_space in action_spaces.values():\naction_space.sample = sample\nreturn action_spaces\ndef _get_observation_spaces(\nself,\nn_nodes,\nn_points,\nagents,\nreveal_cheating_cost,\nreveal_collection_reward,\nscreen_width,\nscreen_height,\n):\n\"\"\"Retrieves observation spaces for all agents.\n        Each observation consist of a list of the point and agent positions as\n        node labels, collected points, an image representing the environment,\n        an action mask representing valid actions for the current agent, and\n        whether the agent can issue the `collect` (-1) action.\n        Args:\n            n_nodes (int): Number of nodes in the graph.\n            n_points (int): Number of points in the graph.\n            agents (list[str]): List of agent names.\n            reveal_cheating_cost (bool): Whether to include cheating cost.\n            reveal_collection_reward (bool): Whether to include collection\n                rewards.\n            screen_width (int): Width of display screen.\n            screen_height (int): Height of display screen.\n        Returns:\n            dict: Dictionary of observation spaces keyed by agent name.\n        \"\"\"\nspaces = {\n# List of node labels, where points/collectors are located.\n\"point_labels\": gymnasium.spaces.Box(\nlow=0, high=n_nodes, shape=(n_points,), dtype=int\n),\n\"collector_labels\": gymnasium.spaces.Box(\nlow=0, high=n_nodes, shape=(len(agents),), dtype=int\n),\n# No. of times each point has been collected.\n\"collected\": gymnasium.spaces.Box(\nlow=0, high=np.inf, shape=(n_points,), dtype=int\n),\n\"image\": gymnasium.spaces.Box(\nlow=0,\nhigh=255,\nshape=(screen_width, screen_height, 3),\ndtype=np.uint8,\n),\n# Action mask for the current agent representing valid\n# actions in the current state.\n\"action_mask\": gymnasium.spaces.Box(\nlow=0, high=1, shape=(n_nodes,), dtype=int\n),\n# Whether it is possible for the current agent to issue\n# the `collect` (-1) action.\n\"collect_action_validity\": gymnasium.spaces.Discrete(n=2, start=0),\n}\nif reveal_cheating_cost:\nspaces[\"cheating_cost\"] = gymnasium.spaces.Box(\nlow=-np.inf, high=np.inf, shape=(n_points,), dtype=np.float64\n)\nif reveal_collection_reward:\nspaces[\"collection_reward\"] = gymnasium.spaces.Box(\nlow=-np.inf, high=np.inf, shape=(n_points,), dtype=np.float64\n)\nobservation_spaces = {\nagent: gymnasium.spaces.Dict(spaces) for agent in agents\n}\nreturn observation_spaces\ndef _get_state_space(\nself, n_nodes, n_points, n_agents, screen_width, screen_height\n):\n\"\"\"Retrieves state space.\n        The global state consists of a list of the point and agent positions\n        as node labels, collected points, and an image representing the\n        environment.\n        Args:\n            n_nodes (int): Number of nodes in the graph.\n            n_points (int): Number of points in the graph.\n            n_agents (int): Number of agents.\n            screen_width (int): Width of display screen.\n            screen_height (int): Height of display screen.\n        Returns:\n            gymnasium.spaces.Dict: State space.\n        \"\"\"\nstate_space = gymnasium.spaces.Dict(\n{\n# List of node labels, where points/collectors are located.\n\"point_labels\": gymnasium.spaces.Box(\nlow=0, high=n_nodes, shape=(n_points,), dtype=int\n),\n\"collector_labels\": gymnasium.spaces.Box(\nlow=0, high=n_nodes, shape=(n_agents,), dtype=int\n),\n# No. of times each point has been collected.\n\"collected\": gymnasium.spaces.Box(\nlow=0, high=np.inf, shape=(n_points,), dtype=int\n),\n\"image\": gymnasium.spaces.Box(\nlow=0,\nhigh=255,\nshape=(screen_width, screen_height, 3),\ndtype=np.uint8,\n),\n\"cheating_cost\": gymnasium.spaces.Box(\nlow=-np.inf,\nhigh=np.inf,\nshape=(n_points,),\ndtype=np.float64,\n),\n\"collection_reward\": gymnasium.spaces.Box(\nlow=-np.inf,\nhigh=np.inf,\nshape=(n_points,),\ndtype=np.float64,\n),\n}\n)\nreturn state_space\ndef _get_node_position(\nself, node_label, nodes_per_row, node_width, node_height\n):\n\"\"\"Returns the position of a node to be displayed on the screen.\n        Args:\n            node_label (int): Node label.\n            nodes_per_row (int): No. of nodes per row.\n            node_width (int): Display width of a node.\n            node_height (int): Display height of a node.\n        Returns:\n            tuple: (x, y) position of the node (with origin at top-left).\n        \"\"\"\nx = (node_label % nodes_per_row) * node_width\ny = (node_label // nodes_per_row) * node_height\nreturn (x, y)\ndef _create_collectors(self, init_agent_labels, agents):\n\"\"\"Creates collector for each agent as a dict.\n        Args:\n            init_agent_labels (list[int]): List of node labels representing\n                initial agent positions.\n            agents (list[str]): List of agent names.\n        Returns:\n            dict: Dictionary of collectors keyed by agent name.\n        \"\"\"\ncollectors = {\nagent: Collector(\npos=self._get_node_position(\nnode_label=label,\nnodes_per_row=self.nodes_per_row,\nnode_width=self.node_width,\nnode_height=self.node_height,\n),\nscaling=0,\ntranslation=0,\nlabel=label,\nid=f\"collector_{agent}\",\n)\nfor agent, label in zip(agents, init_agent_labels)\n}\nreturn collectors\ndef _create_points(self, point_labels):\n\"\"\"Creates points from given node labels.\n        Note that this merges all points at the same node into a single\n        point.\n        Args:\n            point_labels (list[int]): Point positions.\n        Returns:\n            dict: Dictionary of points keyed by node labels.\n        \"\"\"\npoints = {\nlabel: Point(\npos=self._get_node_position(\nnode_label=label,\nnodes_per_row=self.nodes_per_row,\nnode_width=self.node_width,\nnode_height=self.node_height,\n),\nscaling=0,\ntranslation=0,\nlabel=label,\nid=f\"point_{label}\",\n)\nfor label in point_labels\n}\nreturn points\ndef _create_image_array(self, surf, size):\n\"\"\"Returns image array from pygame surface.\n        Args:\n            surf (pygame.Surface): Surface to convert to image array.\n            size (tuple): Tuple of (width, height) to scale surface to.\n        Returns:\n            np.ndarray: Image array.\n        \"\"\"\nscaled_surf = pygame.transform.smoothscale(surf, size)\nreturn np.transpose(\nnp.array(pygame.surfarray.pixels3d(scaled_surf)), axes=(1, 0, 2)\n)\ndef reward(self, cur_node, action):\n\"\"\"Returns reward for executing action in cur_node.\n        If the action is `collect` (-1), we add a reward for collecting the\n        point. However, if the point has already been collected, we add a\n        penalty for cheating.\n        If the action is a label of a new node, the reward is the cost of\n        traversing the edge between the current and new node represented by\n        the weight of the edge.\n        Args:\n            cur_node (int): Node label of current node.\n            action (int): Action to execute, which is either a node label or\n                `collect` (-1).\n        Raises:\n            ValueError: No edge exists between current and new node of action\n                or if the action is `collect` (-1) and cur_node is not a point.\n        Returns:\n            float: Reward\n        \"\"\"\nif action == -1:\nassert cur_node in self.points, (\nf\"Node {cur_node} is not a point. Action cannot be `collect` \"\n\"(-1).\"\n)\n# Add reward for collecting a point, and add penalty if cheating.\nreward = self.collection_reward(cur_node)\nif self.points[cur_node].is_collected():\nreward -= self.cheating_cost(cur_node)\nelse:\ntry:\nreward = -self.graph.adj[cur_node][action][\"weight\"]\nexcept KeyError:\nraise ValueError(\nf\"There is no weighted edge between node {cur_node} and \"\nf\"{action}. Reward cannot be calculated.\"\n)\nreturn reward\ndef _state(\nself,\ngraph,\npoints,\ncollectors,\nreveal_cheating_cost,\nreveal_collection_reward,\n):\n\"\"\"Retrieves state of the current global environment.\n        Args:\n            graph (networkx.Graph): Graph representing the environment.\n            points (dict): Dictionary of points keyed by node labels.\n            collectors (dict): Dictionary of collectors keyed by agent names.\n            reveal_cheating_cost (bool): Whether to reveal cheating cost.\n            reveal_collection_reward (bool): Whether to reveal collection\n                reward.\n        Returns:\n            dict: Current global state.\n        \"\"\"\nstate = {\n\"point_labels\": np.array(\n[point.label for point in points.values()], dtype=int\n),\n\"collector_labels\": np.array(\n[collector.label for collector in collectors.values()],\ndtype=int,\n),\n\"collected\": np.array(\n[point.get_collect_counter() for point in points.values()],\ndtype=int,\n),\n\"image\": self._render(render_mode=\"rgb_array\"),\n}\nif reveal_cheating_cost:\nstate[\"cheating_cost\"] = np.array(\n[self.cheating_cost(point.label) for point in points.values()],\ndtype=np.float64,\n)\nif reveal_collection_reward:\nstate[\"collection_reward\"] = np.array(\n[\nself.collection_reward(point.label)\nfor point in points.values()\n],\ndtype=np.float64,\n)\nreturn state\ndef _get_action_mask(self, agent):\n\"\"\"Retrieves action mask and whether `collect` (-1) can be issued.\n        The action mask is an array representing the validity of each action.\n        An action is valid if the agent can move to the corresponding node.\n        Valid actions are represented by `1`, and invalid actions are\n        represented by `0`.\n        The `collect` (-1) action is valid if the agent is at a point.\n        Args:\n            agent (str): Agent name.\n        Returns:\n            tuple(np.ndarray, bool): Tuple action mask and validity of the\n                `collect` (-1) action.\n        \"\"\"\naction_mask = np.zeros(len(self.graph.nodes), dtype=int)\ncur_node = self.collectors[agent].label\nneighbors = self.graph.neighbors(cur_node)\nfor neighbor in neighbors:\naction_mask[neighbor] = 1\ncollect_action_validity = int(cur_node in self.points)\nreturn action_mask, collect_action_validity\ndef observe(self, agent):\n# FIXME: Warning for api_test /Users/lfwa/Library/Caches/pypoetry/\n# virtualenvs/collector-gjPrMD7k-py3.10/lib/python3.10/site-packages/\n# pettingzoo/test/api_test.py:60: UserWarning: Observation is not\n# NumPy array\n# warnings.warn(\"Observation is not NumPy array\")\nobs = self._state(\nself.graph,\nself.points,\nself.collectors,\nself.reveal_cheating_cost,\nself.reveal_collection_reward,\n)\n(\nobs[\"action_mask\"],\nobs[\"collect_action_validity\"],\n) = self._get_action_mask(agent)\nreturn obs\ndef state(self):\nreturn self._state(\nself.graph, self.points, self.collectors, True, True\n)\ndef reset(self, seed=None, return_info=False, options=None):\nif seed is not None:\nself.seed(seed)\nself.agents = self.possible_agents[:]\nself._agent_selector.reinit(self.agents)\nself.agent_selection = self._agent_selector.reset()\nself.collectors = self._create_collectors(\nself.init_agent_labels, self.agents\n)\nself.points = self._create_points(self._point_labels)\nself.iteration = 0\nself.total_points_collected = 0\nself.has_reset = True\nself.terminate = False\nself.truncate = False\nself.rewards = {agent: 0 for agent in self.agents}\nself._cumulative_rewards = {agent: 0 for agent in self.agents}\nself.terminations = {agent: False for agent in self.agents}\nself.truncations = {agent: False for agent in self.agents}\nself.infos = {agent: {} for agent in self.agents}\nself.cumulative_rewards = {agent: 0 for agent in self.agents}\nobservations = {agent: self.observe(agent) for agent in self.agents}\nif not return_info:\nreturn observations\nelse:\nreturn observations, self.infos\ndef step(self, action):\nassert (\nself.has_reset\n), \"Environment has not been reset yet. Call env.reset() first.\"\nagent = self.agent_selection\nif self.terminations[agent] or self.truncations[agent]:\nself._was_dead_step(action)\n# Guard against first agent dying first since _was_dead_step()\n# does not update agent_selection when that happens.\nif self.agent_selection not in self.agents and self.agents:\nself.agent_selection = self._agent_selector.next()\nreturn\nif (\nnot self.action_space(agent).contains(action)\nand action is not None\n):\nraise ValueError(\nf\"Action {action} is not in the action space for \"\nf\"agent {agent}.\"\n)\naction_mask, collect_action_validity = self._get_action_mask(agent)\ncollector = self.collectors[agent]\ncur_node = collector.label\nif (\naction is not None\nand action_mask[action]\nor (action == -1 and collect_action_validity)\n):\nreward = self.reward(cur_node, action)\nif action == -1:\n# Collect point.\ncollector.collect(\nself.points[cur_node], self.total_points_collected\n)\nself.total_points_collected += 1\nelse:\n# Move agent to the new node label.\ncollector.move(\nposition=self._get_node_position(\nnode_label=action,\nnodes_per_row=self.nodes_per_row,\nnode_width=self.node_width,\nnode_height=self.node_height,\n),\nlabel=action,\n)\nelse:\nreward = 0\n# Update termination and truncation for agent.\nif (\nself.collectors[agent].total_points_collected\n&gt;= self.max_collect[agent]\n):\nself.terminations[agent] = True\nself.terminate = all(self.terminations.values())\nself.truncate = all(self.truncations.values())\nself.iteration += 1\nself.rewards[agent] = reward\nself.cumulative_rewards[agent] += reward\n# Cumulative reward since agent has last acted.\nself._cumulative_rewards[agent] = 0\nself._accumulate_rewards()\nself.agent_selection = self._agent_selector.next()\nif self.render_mode == \"human\":\nself.render()\ndef render(self):\nassert (\nself.has_reset\n), \"Environment has not been reset yet. Call env.reset() first.\"\nif self.render_mode is None:\ngymnasium.logger.warn(\nf\"No render mode specified, skipping render. Please \"\n\"specify render_mode as one of the supported modes \"\nf\"{self.metadata['render_modes']} at initialization.\"\n)\nelse:\nreturn self._render(render_mode=self.render_mode)\ndef _render(self, render_mode):\n\"\"\"Renders the environment.\n        Args:\n            render_mode (str): One of the supported render modes.\n        Returns:\n            np.ndarray or None: Returns the rendered image if render_mode is\n                `rgb_array`, otherwise returns None.\n        \"\"\"\npygame.font.init()\nif self.screen is None and render_mode == \"human\":\npygame.init()\npygame.display.init()\ntry:\nself.screen = pygame.display.set_mode(\n(SCREEN_WIDTH, SCREEN_HEIGHT)\n)\nexcept Exception as e:\nerror_msg = f\"Could not initialize pygame display: {e}.\"\nif self.dynamic_display:\nerror_msg += \" Try disabling `dynamic_display`.\"\ngymnasium.logger.error(error_msg)\nraise e\nif self.clock is None:\nself.clock = pygame.time.Clock()\nself.surf = pygame.Surface((SCREEN_WIDTH, SCREEN_HEIGHT))\n# Add white background.\nself.surf.fill((255, 255, 255))\nself._render_obstacles(\nsurf=self.surf,\nnodes=self.graph.nodes,\nnodes_per_row=self.nodes_per_row,\nnode_width=self.node_width,\nnode_height=self.node_height,\n)\nself._render_points(\nsurf=self.surf,\npoints=self.points,\npoint_size=POINT_SIZE,\n)\nself._render_paths(\nsurf=self.surf,\ncollectors=self.collectors,\npath_size=PATH_SIZE,\n)\nself._render_collectors(\nsurf=self.surf,\ncollectors=self.collectors,\ncollector_len=COLLECTOR_LEN,\ncollector_size=COLLECTOR_SIZE,\n)\nself._render_text(self.surf)\nif render_mode == \"human\":\npygame.event.pump()\nself.clock.tick(self.metadata[\"render_fps\"])\nassert self.screen is not None\nself.screen.blit(self.surf, (0, 0))\npygame.display.update()\nelif render_mode == \"rgb_array\":\nreturn self._create_image_array(\nself.surf, (SCREEN_WIDTH, SCREEN_HEIGHT)\n)\ndef _render_text(self, surf):\n\"\"\"Renders information text, e.g. stats about environment and actions.\n        Args:\n            surf (pygame.Surface): Surface to render text on.\n        \"\"\"\n(\nstats,\noverall_total_points_collected,\noverall_unique_points_collected,\noverall_cheated,\n) = self._get_stats()\ntotal_reward = sum(self.cumulative_rewards.values())\nfont = pygame.font.Font(pygame.font.get_default_font(), FONT_SIZE)\ntext1 = font.render(\n(\nf\"Iteration: {self.iteration} | \"\nf\"Total points collected: {overall_total_points_collected} | \"\n\"Unique points collected: \"\nf\"{overall_unique_points_collected} / {len(self.points)} | \"\nf\"Cheated: {overall_cheated}\"\n),\nTrue,\n(0, 0, 255),\n)\ntext2 = font.render(\nf\"Total cumulative reward: {total_reward}\",\nTrue,\n(0, 0, 255),\n)\nsurf.blit(text1, (10, 10))\nsurf.blit(text2, (10, 40))\ndef _get_stats(self):\n\"\"\"Retrieves stats for all collectors.\n        Returns:\n            tuple: Tuple of stats.\n        \"\"\"\nstats = {}\noverall_total_points_collected = 0\noverall_unique_points_collected = 0\noverall_cheated = 0\nfor agent in self.collectors:\ncollector = self.collectors[agent]\nstats[agent] = {\n\"total_points_collected\": collector.total_points_collected,\n\"unique_points_collected\": collector.unique_points_collected,\n\"cheated\": collector.cheated,\n}\noverall_total_points_collected += collector.total_points_collected\noverall_unique_points_collected += (\ncollector.unique_points_collected\n)\noverall_cheated += collector.cheated\nreturn (\nstats,\noverall_total_points_collected,\noverall_unique_points_collected,\noverall_cheated,\n)\ndef _render_obstacles(\nself, surf, nodes, nodes_per_row, node_width, node_height\n):\n\"\"\"Renders obstacles (nodes w. no neighbors) as black rectangles.\n        Args:\n            surf (pygame.Surface): Surface to render obstacles on.\n            nodes (list): List of nodes.\n            nodes_per_row (int): No. of nodes to display per row.\n            node_width (int): Display width of a node.\n            node_height (int): Display height of a node.\n        \"\"\"\n# For static graphs we only have to render obstacles once.\nif self.cached_obstacle_surf is None or not self.static_graph:\ncached_obstacle_surf = pygame.Surface(\n(SCREEN_WIDTH, SCREEN_HEIGHT)\n)\n# Add white background.\ncached_obstacle_surf.fill((255, 255, 255))\nfor node in nodes:\nif any(True for _ in self.graph.neighbors(node)):\ncontinue\nx, y = self._get_node_position(\nnode_label=node,\nnodes_per_row=nodes_per_row,\nnode_width=node_width,\nnode_height=node_height,\n)\nrect = pygame.Rect(x, y, node_width, node_height)\npygame.draw.rect(\ncached_obstacle_surf,\ncolor=(0, 0, 0),\nrect=rect,\n)\nself.cached_obstacle_surf = cached_obstacle_surf\nsurf.blit(self.cached_obstacle_surf, (0, 0))\ndef _render_points(self, surf, points, point_size):\n\"\"\"Renders all points as circles.\n        Points are colored according to their collector as a pie chart.\n        Args:\n            surf (pygame.Surface): Surface to render points on.\n            points (list[Points]): List of points to render.\n            point_size (int): Render size of a point.\n        \"\"\"\nfor point in points.values():\nx, y = self._center(point.position)\nbounding_box = pygame.Rect(\nx - point_size / 2, y - point_size / 2, point_size, point_size\n)\ntotal_collections = point.get_collect_counter()\nstart_angle = 0\nif total_collections == 0:\npygame.draw.circle(\nsurf,\npoint.color,\n(x, y),\npoint_size / 2,\n)\nelse:\nfor (\ncollector_id,\ncollections,\n) in point.collector_tracker.items():\nif collections == 0:\ncontinue\narc_length = collections / total_collections * 2 * math.pi\npygame.draw.arc(\nsurf,\nself.collectors[collector_id[10:]].color,\nbounding_box,\nstart_angle,\nstart_angle + arc_length,\npoint_size,\n)\nstart_angle += arc_length\ndef _render_paths(self, surf, collectors, path_size):\n\"\"\"Renders paths taken by collectors.\n        Colors are assigned to paths based on the collector that took it.\n        If paths overlap then they are colored in segments.\n        Args:\n            surf (pygame.Surface): Surface to render paths on.\n            collectors (dict): Dict of collectors.\n            path_size (int): Render size of paths.\n        \"\"\"\npath_pairs = {}\nfor collector in collectors.values():\npath_pos_len = len(collector.path_positions)\nif path_pos_len &lt; 2:\ncontinue\nfor i in range(1, path_pos_len):\nkey = (\ncollector.path_positions[i - 1],\ncollector.path_positions[i],\n)\nreverse_key = (key[1], key[0])\n# We should not care whether it is (a, b) or (b, a).\nif key in path_pairs:\npath_pairs[key] += [collector]\nelif reverse_key in path_pairs:\npath_pairs[reverse_key] += [collector]\nelse:\npath_pairs[key] = [collector]\nfor path, collectors in path_pairs.items():\ntotal_collectors = len(collectors)\nprev_x, prev_y = self._center(path[0])\nx, y = self._center(path[1])\nsegment_x = (x - prev_x) / total_collectors\nsegment_y = (y - prev_y) / total_collectors\nfor collector in collectors:\npygame.draw.line(\nsurf,\ncollector.color,\n(prev_x, prev_y),\n(prev_x + segment_x, prev_y + segment_y),\npath_size,\n)\nprev_x += segment_x\nprev_y += segment_y\ndef _render_collectors(\nself,\nsurf,\ncollectors,\ncollector_len,\ncollector_size,\n):\n\"\"\"Renders all collectors as crosses.\n        Collectors are rotated when stacked to avoid overlapping.\n        Black borders are added to crosses.\n        Args:\n            surf (pygame.Surface): Surface to render collectors on.\n            collectors (dict): Dict of collectors.\n            collector_len (int): Length of collector cross.\n            collector_size (int): Size of collector cross.\n        \"\"\"\nfor position, colls in groupby(\ncollectors.values(), lambda col: col.position\n):\nposition = self._center(position)\ncolls = list(colls)\ntotal_collectors = len(colls)\nshift_increment = collector_len / total_collectors\nshift = collector_len / 2\nfor i, collector in enumerate(colls):\ncross_rotate_shift = i * shift_increment\n# Add black border to cross.\nborder_size = math.ceil(collector_size * 1.7)\npygame.draw.line(\nsurf,\n(0, 0, 0),\nstart_pos=(\nposition[0] + cross_rotate_shift - shift,\nposition[1] - shift,\n),\nend_pos=(\nposition[0] + shift - cross_rotate_shift,\nposition[1] + shift,\n),\nwidth=border_size,\n)\npygame.draw.line(\nsurf,\n(0, 0, 0),\nstart_pos=(\nposition[0] + shift,\nposition[1] + cross_rotate_shift - shift,\n),\nend_pos=(\nposition[0] - shift,\nposition[1] + shift - cross_rotate_shift,\n),\nwidth=border_size,\n)\n# Draw cross.\npygame.draw.line(\nsurf,\ncollector.color,\nstart_pos=(\nposition[0] + cross_rotate_shift - shift,\nposition[1] - shift,\n),\nend_pos=(\nposition[0] + shift - cross_rotate_shift,\nposition[1] + shift,\n),\nwidth=collector_size,\n)\npygame.draw.line(\nsurf,\ncollector.color,\nstart_pos=(\nposition[0] + shift,\nposition[1] + cross_rotate_shift - shift,\n),\nend_pos=(\nposition[0] - shift,\nposition[1] + shift - cross_rotate_shift,\n),\nwidth=collector_size,\n)\ndef _center(self, position):\n\"\"\"Returns the position centered on the node.\n        Args:\n            pos (tuple): Position to center.\n        Returns:\n            tuple: Centered position.\n        \"\"\"\nreturn (\nposition[0] + self.node_width / 2,\nposition[1] + self.node_height / 2,\n)\ndef observation_space(self, agent):\nreturn self.observation_spaces[agent]\ndef action_space(self, agent):\nreturn self.action_spaces[agent]\ndef seed(self, seed=None):\nself.rng, seed = gymnasium.utils.seeding.np_random(seed)\nreturn [seed]\ndef close(self):\nif self.screen is not None:\npygame.display.quit()\nself.isopen = False\npygame.quit()\n</code></pre>"},{"location":"api/environments/#datadynamics.environments.graph_collector.graph_collector.raw_env.__init__","title":"<code>__init__(graph, point_labels, init_agent_labels, max_collect, nodes_per_row=None, cheating_cost=lambda point_label: 500 * 0.5, collection_reward=lambda point_label: 100, reveal_cheating_cost=True, reveal_collection_reward=True, static_graph=True, dynamic_display=False, seed=None, render_mode=None)</code>","text":"<p>Initializes the graph collector environment.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>networkx.Graph</code> <p>Input directed or undirected graph defining the environment. Node labels must be a continuous set of integers starting at 0.</p> required <code>point_labels</code> <code>list[int]</code> <p>List of node labels to identify collectable points. All duplicate labels will be merged when creating points in the environment.</p> required <code>init_agent_labels</code> <code>list[int]</code> <p>List of node labels to identify initial agent positions.</p> required <code>max_collect</code> <code>list[int]</code> <p>List of maximum number of points each agent can collect.</p> required <code>nodes_per_row</code> <code>int</code> <p>Number of nodes to display per row. Defaults to None.</p> <code>None</code> <code>cheating_cost</code> <code>function</code> <p>Function that takes a node label and returns the cost of cheating by collecting an already collected point. Influences reward for collecting points. Defaults to lambda node_label: 500 * 0.5.</p> <code>lambda point_label: 500 * 0.5</code> <code>collection_reward</code> <code>function</code> <p>Function that takes a point label and returns the reward for collecting that point. Defaults to lambda point_label: 100.</p> <code>lambda point_label: 100</code> <code>reveal_cheating_cost</code> <code>bool</code> <p>Whether to reveal the cheating costs to the agent in observations. Defaults to True.</p> <code>True</code> <code>reveal_collection_reward</code> <code>bool</code> <p>Whether to reveal the collection rewards to the agent in observations. Defaults to True.</p> <code>True</code> <code>static_graph</code> <code>bool</code> <p>Whether the underlying graph is static and never changes. Significantly impacts performance since everything in the environment have to be re-rendered for every frame to ensure consistency. May also influence the performance of policies as e.g. shortest paths will need to be recomputed for every action to determine optimal agent movement. Defaults to True.</p> <code>True</code> <code>dynamic_display</code> <code>bool</code> <p>Whether to dynamically adjust the display size to the graph size. Defaults to False.</p> <code>False</code> <code>seed</code> <code>int</code> <p>Seed for random number generator. Defaults to None.</p> <code>None</code> <code>render_mode</code> <code>str</code> <p>Render mode. Supported modes are specified in environment's metadata[\"render_modes\"] dict. Defaults to None.</p> <code>None</code> Source code in <code>datadynamics/environments/graph_collector/graph_collector.py</code> <pre><code>def __init__(\nself,\ngraph,\npoint_labels,\ninit_agent_labels,\nmax_collect,\nnodes_per_row=None,\ncheating_cost=lambda point_label: 500 * 0.5,\ncollection_reward=lambda point_label: 100,\nreveal_cheating_cost=True,\nreveal_collection_reward=True,\nstatic_graph=True,\ndynamic_display=False,\nseed=None,\nrender_mode=None,\n):\n\"\"\"Initializes the graph collector environment.\n    Args:\n        graph (networkx.Graph): Input directed or undirected graph\n            defining the environment. Node labels must be a continuous set\n            of integers starting at 0.\n        point_labels (list[int]): List of node labels to identify\n            collectable points. All duplicate labels will be merged when\n            creating points in the environment.\n        init_agent_labels (list[int]): List of node labels to identify\n            initial agent positions.\n        max_collect (list[int]): List of maximum number of points each\n            agent can collect.\n        nodes_per_row (int, optional): Number of nodes to display per row.\n            Defaults to None.\n        cheating_cost (function, optional): Function that takes a node\n            label and returns the cost of cheating by collecting an\n            already collected point. Influences reward for collecting\n            points. Defaults to lambda node_label: 500 * 0.5.\n        collection_reward (function, optional): Function that takes a point\n            label and returns the reward for collecting that point.\n            Defaults to lambda point_label: 100.\n        reveal_cheating_cost (bool, optional): Whether to reveal the\n            cheating costs to the agent in observations. Defaults to True.\n        reveal_collection_reward (bool, optional): Whether to reveal the\n            collection rewards to the agent in observations. Defaults to\n            True.\n        static_graph (bool, optional): Whether the underlying graph is\n            static and never changes. Significantly impacts performance\n            since everything in the environment have to be re-rendered\n            for every frame to ensure consistency. May also influence the\n            performance of policies as e.g. shortest paths will need to be\n            recomputed for every action to determine optimal agent\n            movement. Defaults to True.\n        dynamic_display (bool, optional): Whether to dynamically adjust\n            the display size to the graph size. Defaults to False.\n        seed (int, optional): Seed for random number generator. Defaults\n            to None.\n        render_mode (str, optional): Render mode. Supported modes are\n            specified in environment's metadata[\"render_modes\"] dict.\n            Defaults to None.\n    \"\"\"\ngymnasium.logger.info(\"Initializing graph collector environment...\")\nassert (\nrender_mode in self.metadata[\"render_modes\"] or render_mode is None\n), (\nf\"render_mode: {render_mode} is not supported. \"\nf\"Supported modes: {self.metadata['render_modes']}\"\n)\ngymnasium.logger.info(\" - Validating input positions...\")\n# Check that all agent and point labels are nodes in the graph.\nassert all(\nlabel in graph.nodes for label in init_agent_labels\n), \"Agent labels must be nodes in the graph!\"\nassert all(\nlabel in graph.nodes for label in point_labels\n), \"Point labels must be nodes in the graph!\"\n# Disallow agents to spawn inside obstacles as they would\n# not be able to move, i.e., agent labels must have neighbors.\nassert all(\nany(True for _ in graph.neighbors(agent_label))\nfor agent_label in init_agent_labels\n), \"Agent labels must not encode obstacles!\"\n# Allow but issue warning for points inside obstacles as they just\n# cannot be collected.\nif any(\nnot any(True for _ in graph.neighbors(point_label))\nfor point_label in point_labels\n):\ngymnasium.logger.warn(\n\"Some points are inside obstacles and cannot be collected!\"\n)\n# Warn if points are overlapping.\nif len(point_labels) != len(set(point_labels)):\ngymnasium.logger.warn(\"Some points overlap and will be merged!\")\ngymnasium.logger.info(\" - Seeding environment...\")\nself.seed(seed)\nself.graph = graph\n# Remove duplicate points.\nself._point_labels = list(dict.fromkeys(point_labels))\nself.init_agent_labels = init_agent_labels\nself.render_mode = render_mode\nself.cheating_cost = cheating_cost\nself.collection_reward = collection_reward\nself.reveal_cheating_cost = reveal_cheating_cost\nself.reveal_collection_reward = reveal_collection_reward\nself.static_graph = static_graph\nif nodes_per_row is None:\nnodes_per_row = math.ceil(math.sqrt(len(self.graph.nodes)))\nself.nodes_per_row = nodes_per_row\n# For dynamic displaying, we adjust the screen width s.t.\n# nodes are displayed as squares.\nif dynamic_display:\nglobal SCREEN_WIDTH\nrows = math.ceil(len(self.graph.nodes) / nodes_per_row)\naspect_ratio = self.nodes_per_row / rows\nSCREEN_WIDTH = int(SCREEN_HEIGHT * aspect_ratio)\nself.node_width, self.node_height = self._get_node_shape(\nlen(self.graph.nodes),\nself.nodes_per_row,\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\nself.reward_range = (-np.inf, 0)\nself.agents = [\nf\"agent_{i}\" for i in range(len(self.init_agent_labels))\n]\nself.possible_agents = self.agents[:]\nself.agent_name_mapping = {\nagent: i for i, agent in enumerate(self.agents)\n}\nself._agent_selector = agent_selector(self.agents)\nself.max_collect = {\nagent: max_collect[i] for i, agent in enumerate(self.agents)\n}\ngymnasium.logger.info(\" - Setting up action spaces...\")\nself.action_spaces = self._get_action_spaces(\nself.agents, self.graph.nodes\n)\ngymnasium.logger.info(\" - Setting up observation spaces...\")\nself.observation_spaces = self._get_observation_spaces(\nlen(self.graph.nodes),\nlen(self._point_labels),\nself.agents,\nself.reveal_cheating_cost,\nself.reveal_collection_reward,\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\ngymnasium.logger.info(\" - Setting up state space...\")\nself.state_space = self._get_state_space(\nlen(self.graph.nodes),\nlen(self._point_labels),\nlen(self.agents),\nSCREEN_WIDTH,\nSCREEN_HEIGHT,\n)\n# The following are set in reset().\nself.iteration = 0\nself.total_points_collected = 0\nself.points = None\nself.agent_selection = None\nself.has_reset = False\nself.terminate = False\nself.truncate = False\n# Dicts with agent as key.\nself.rewards = None\nself._cumulative_rewards = None\nself.terminations = None\nself.truncations = None\nself.infos = None\nself.collectors = None\nself.cumulative_rewards = None\n# pygame\nself.screen = None\nself.clock = None\nself.surf = None\nself.cached_obstacle_surf = None\nself.isopen = False\ngymnasium.logger.info(\"Environment initialized.\")\n</code></pre>"},{"location":"api/environments/#datadynamics.environments.graph_collector.graph_collector.raw_env.reward","title":"<code>reward(cur_node, action)</code>","text":"<p>Returns reward for executing action in cur_node.</p> <p>If the action is <code>collect</code> (-1), we add a reward for collecting the point. However, if the point has already been collected, we add a penalty for cheating. If the action is a label of a new node, the reward is the cost of traversing the edge between the current and new node represented by the weight of the edge.</p> <p>Parameters:</p> Name Type Description Default <code>cur_node</code> <code>int</code> <p>Node label of current node.</p> required <code>action</code> <code>int</code> <p>Action to execute, which is either a node label or <code>collect</code> (-1).</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>No edge exists between current and new node of action or if the action is <code>collect</code> (-1) and cur_node is not a point.</p> <p>Returns:</p> Name Type Description <code>float</code> <p>Reward</p> Source code in <code>datadynamics/environments/graph_collector/graph_collector.py</code> <pre><code>def reward(self, cur_node, action):\n\"\"\"Returns reward for executing action in cur_node.\n    If the action is `collect` (-1), we add a reward for collecting the\n    point. However, if the point has already been collected, we add a\n    penalty for cheating.\n    If the action is a label of a new node, the reward is the cost of\n    traversing the edge between the current and new node represented by\n    the weight of the edge.\n    Args:\n        cur_node (int): Node label of current node.\n        action (int): Action to execute, which is either a node label or\n            `collect` (-1).\n    Raises:\n        ValueError: No edge exists between current and new node of action\n            or if the action is `collect` (-1) and cur_node is not a point.\n    Returns:\n        float: Reward\n    \"\"\"\nif action == -1:\nassert cur_node in self.points, (\nf\"Node {cur_node} is not a point. Action cannot be `collect` \"\n\"(-1).\"\n)\n# Add reward for collecting a point, and add penalty if cheating.\nreward = self.collection_reward(cur_node)\nif self.points[cur_node].is_collected():\nreward -= self.cheating_cost(cur_node)\nelse:\ntry:\nreward = -self.graph.adj[cur_node][action][\"weight\"]\nexcept KeyError:\nraise ValueError(\nf\"There is no weighted edge between node {cur_node} and \"\nf\"{action}. Reward cannot be calculated.\"\n)\nreturn reward\n</code></pre>"},{"location":"api/environments/#datadynamics.environments.graph_collector.graph_collector.env","title":"<code>env(**kwargs)</code>","text":"<p>Creates a graph collector environment.</p> <p>Returns:</p> Type Description <p>pettingzoo.utils.env.AECEnv: Created environment.</p> Source code in <code>datadynamics/environments/graph_collector/graph_collector.py</code> <pre><code>def env(**kwargs):\n\"\"\"Creates a graph collector environment.\n    Returns:\n        pettingzoo.utils.env.AECEnv: Created environment.\n    \"\"\"\nenv = raw_env(**kwargs)\nreturn env\n</code></pre>"},{"location":"api/metrics/","title":"Metrics","text":"<p>Datadynamics contains several metrics for comparing similarity between simulations.</p>"},{"location":"api/metrics/#optimal-transport-dataset-distance-otdd","title":"Optimal Transport Dataset Distance (OTDD)","text":""},{"location":"api/metrics/#datadynamics.utils.metrics.otdd.otdd","title":"<code>otdd(d1_collections_filename, d2_collections_filename, include_timestamps=True)</code>","text":"<p>Optimal transport dataset distance between two collections over time.</p> <p>The collections must be of equal length and created using datadynamics.utils.post_processing.save_collections. We use Microsoft's OTDD library to compute the distance between the collections for each timestamp to see how the distance changes over time during the simulation.</p> Warning <p>This function requires the OTDD library to be installed which is not included by default in datadynamics.</p> Note <p>We skip any timestamps for which the distance cannot be computed. Also, the OTDD values will likely not be affected by whether or not timestamps are included in the input features.</p> <p>Parameters:</p> Name Type Description Default <code>d1_collections_filename</code> <code>str</code> <p>The filename of the first collection.</p> required <code>d2_collections_filename</code> <code>str</code> <p>The filename of the second collection.</p> required <code>include_timestamps</code> <code>bool</code> <p>Whether to include timestamps in the input features. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple of two lists. The first list contains the timestamps for which the distance was computed. The second list contains the distances for each timestamp.</p> Source code in <code>datadynamics/utils/metrics/otdd.py</code> <pre><code>def otdd(\nd1_collections_filename, d2_collections_filename, include_timestamps=True\n):\n\"\"\"Optimal transport dataset distance between two collections over time.\n    The collections must be of equal length and created using\n    datadynamics.utils.post_processing.save_collections. We use Microsoft's\n    OTDD library to compute the distance between the collections for each\n    timestamp to see how the distance changes over time during the simulation.\n    Warning:\n        This function requires the OTDD library to be installed which is not\n        included by default in datadynamics.\n    Note:\n        We skip any timestamps for which the distance cannot be computed.\n        Also, the OTDD values will likely not be affected by whether or not\n        timestamps are included in the input features.\n    Args:\n        d1_collections_filename (str): The filename of the first collection.\n        d2_collections_filename (str): The filename of the second collection.\n        include_timestamps (bool, optional): Whether to include timestamps in\n            the input features. Defaults to True.\n    Returns:\n        tuple: A tuple of two lists. The first list contains the timestamps\n            for which the distance was computed. The second list contains the\n            distances for each timestamp.\n    \"\"\"\nwith open(d1_collections_filename, \"rb\") as f:\nd1_collections = pickle.load(f)\nwith open(d2_collections_filename, \"rb\") as f:\nd2_collections = pickle.load(f)\nd1_timestamps, d1_feats, d1_targets = extract.feats_targets_timestamps(\nd1_collections, include_timestamps\n)\nd2_timestamps, d2_feats, d2_targets = extract.feats_targets_timestamps(\nd2_collections, include_timestamps\n)\nn1, n2 = len(d1_timestamps), len(d2_timestamps)\nassert n1 == n2, \"The collections must be of equal length.\"\ncompleted_timestamps = []\ndistances = []\nfor i in tqdm.tqdm(range(1, n1 + 1), desc=\"Computing OTDD\"):\ntry:\nd1 = dataset_from_numpy(d1_feats[:i], d1_targets[:i])\nd2 = dataset_from_numpy(d2_feats[:i], d2_targets[:i])\ndist = DatasetDistance(d1, d2, inner_ot_method=\"exact\")\nd = dist.distance(maxsamples=1000)\ndistances.append(d.item())\ncompleted_timestamps.append(d1_timestamps[i - 1])\nexcept Exception as e:\nprint(f\"Skipping {i} due to {e}...\")\ncontinue\nreturn completed_timestamps, distances\n</code></pre>"},{"location":"api/policies/","title":"Policies","text":""},{"location":"api/policies/#greedy-policy","title":"Greedy Policy","text":""},{"location":"api/policies/#datadynamics.policies.greedy_policy.greedy_policy.GraphGreedyPolicy","title":"<code>GraphGreedyPolicy</code>","text":"<p>         Bases: <code>BasePolicy</code></p> <p>Greedy policy for graph environment using shortest path.</p> <p>Compatible only with graph_collector environment.</p> Note <p>This is very slow for non-static graphs as shortest paths are computed in O(V^3) time for every action search. Static graphs use cached shortest paths.</p> <p>Attributes:</p> Name Type Description <code>env</code> <code>pettingzoo.utils.env.AECEnv</code> <p>Environment used by policy.</p> <code>shortest_len_paths</code> <code>dict</code> <p>Cached shortest paths including path lengths for all node pairs.</p> <code>cur_goals</code> <code>dict</code> <p>Cached goals for each agent consisting of (path, collected, point_idx) tuples keyed by agent name.</p> Source code in <code>datadynamics/policies/greedy_policy/greedy_policy.py</code> <pre><code>class GraphGreedyPolicy(BasePolicy):\n\"\"\"Greedy policy for graph environment using shortest path.\n    Compatible only with graph_collector environment.\n    Note:\n        This is very slow for non-static graphs as shortest paths are\n        computed in O(V^3) time for every action search.\n        Static graphs use cached shortest paths.\n    Attributes:\n        env (pettingzoo.utils.env.AECEnv): Environment used by policy.\n        shortest_len_paths (dict): Cached shortest paths including path\n            lengths for all node pairs.\n        cur_goals (dict): Cached goals for each agent consisting of\n            (path, collected, point_idx) tuples keyed by agent name.\n    \"\"\"\ndef __init__(self, env):\nassert env.metadata[\"name\"] == \"graph_collector\", (\nf\"{self.__class__.__name__} is only compatible with \"\n\"graph_collector.\"\n)\ngymnasium.logger.info(\"Initializing GraphGreedyPolicy...\")\nself.env = env\nif self.env.static_graph:\ngymnasium.logger.info(\n\" - Computing and caching shortest paths. This runs in O(V^3) \"\n\"and may take a while...\"\n)\nself.shortest_len_paths = dict(\nnx.all_pairs_dijkstra(self.env.graph)\n)\n# Shortest len paths is a dict with node labels as keys and values\n# consisting of a (length dict, path dict) tuple containing\n# shortest paths between all pairs of nodes.\nself.point_labels = set()\n# cur_goals consist of (path, collected, point_idx) keyed by agent.\nself.cur_goals = {}\ngymnasium.logger.info(\"Completed initialization.\")\ndef action(self, observation, agent):\nif self.env.terminations[agent] or self.env.truncations[agent]:\n# Agent is dead, the only valid action is None.\nreturn None\nif not self.env.static_graph:\ngymnasium.logger.info(\n\"Recomputing shortest paths in O(V^3) for non-static graph...\"\n)\nself.shortest_len_paths = dict(\nnx.all_pairs_dijkstra(self.env.graph)\n)\nself.cur_goals = {}\nself.point_labels = set()\nif not self.point_labels:\n# For static graphs, the points should not change or change\n# position as such we only need to compute labels once.\nself.point_labels = set(observation[\"point_labels\"])\nagent_idx = int(agent[-1])\ncur_node = observation[\"collector_labels\"][agent_idx]\ngoal_path, goal_collected, goal_point_idx = self.cur_goals.get(\nagent, ([], None, None)\n)\n# Update goal if we completed the goal (goal_path is empty) or if\n# the goal was collected by another agent meanwhile.\nif (\nnot goal_path\nor goal_collected != observation[\"collected\"][goal_point_idx]\n):\nbest_reward = -np.inf\nfor i, point_label in enumerate(observation[\"point_labels\"]):\npath = self.shortest_len_paths.get(cur_node, ({}, {}))[1].get(\npoint_label, []\n)\nif not path:\ncontinue\ncollected = observation[\"collected\"][i]\nreward = -self.shortest_len_paths.get(cur_node, ({}, {}))[\n0\n].get(point_label, np.inf)\nif \"collection_reward\" in observation:\nreward += observation[\"collection_reward\"][i]\nif \"cheating_cost\" in observation and collected &gt; 0:\nreward -= observation[\"cheating_cost\"][i]\nif reward &gt; best_reward:\nbest_reward = reward\n# Trim current node and add a `collect` action.\ngoal_path = path[1:] + [-1]\ngoal_collected = collected\ngoal_point_idx = i\nself.cur_goals[agent] = (goal_path, goal_collected, goal_point_idx)\naction = goal_path.pop(0) if goal_path else None\nif action is None:\ngymnasium.logger.warn(\nf\"{agent} cannot reach any points and will issue None \"\n\"actions.\"\n)\nreturn action\n</code></pre>"},{"location":"api/policies/#datadynamics.policies.greedy_policy.greedy_policy.GreedyPolicy","title":"<code>GreedyPolicy</code>","text":"<p>         Bases: <code>BasePolicy</code></p> <p>Greedy policy for collector environment.</p> <p>This policy computes the expected reward for every action in every step and chooses the one with the highest expected reward.</p> <p>Compatible only with collector environment.</p> Note <p>This is only locally optimal and not globally. Best routes to collect all points are disregarded and we only search for the next best point for every step. The policy may degenerate and always sample the same point if the cost of cheating is lower than the reward for collecting a point.</p> <p>Attributes:</p> Name Type Description <code>env</code> <code>pettingzoo.utils.env.AECEnv</code> <p>Environment used by policy.</p> Source code in <code>datadynamics/policies/greedy_policy/greedy_policy.py</code> <pre><code>class GreedyPolicy(BasePolicy):\n\"\"\"Greedy policy for collector environment.\n    This policy computes the expected reward for every action in every step\n    and chooses the one with the highest expected reward.\n    Compatible only with collector environment.\n    Note:\n        This is only locally optimal and not globally. Best routes to collect\n        all points are disregarded and we only search for the next best point\n        for every step.\n        The policy may degenerate and always sample the same point if the cost\n        of cheating is lower than the reward for collecting a point.\n    Attributes:\n        env (pettingzoo.utils.env.AECEnv): Environment used by policy.\n    \"\"\"\ndef __init__(self, env):\nassert env.metadata[\"name\"] == \"collector\", (\nf\"{self.__class__.__name__} is only compatible with \" \"collector.\"\n)\nself.env = env\ndef action(self, observation, agent):\nif self.env.terminations[agent] or self.env.truncations[agent]:\n# Agent is dead, the only valid action is None.\nreturn None\nagent_idx = int(agent[-1])\ncur_position = observation[\"collector_positions\"][agent_idx]\nbest_reward = -np.inf\nbest_action = None\nfor i, position in enumerate(observation[\"point_positions\"]):\ncheating = observation[\"collected\"][i] &gt; 0\nreward = -np.linalg.norm(cur_position - position)\nif \"collection_reward\" in observation:\nreward += observation[\"collection_reward\"][i]\nif \"cheating_cost\" in observation and cheating:\nreward -= observation[\"cheating_cost\"][i]\nif reward &gt; best_reward:\nbest_reward = reward\nbest_action = i\nif best_action is None:\ngymnasium.logger.warn(\nf\"{agent} cannot reach any points and will issue None \"\n\"actions.\"\n)\nreturn best_action\n</code></pre>"},{"location":"api/policies/#datadynamics.policies.greedy_policy.greedy_policy.policy","title":"<code>policy(**kwargs)</code>","text":"<p>Creates a suitable greedy policy for a given environment.</p> <p>Returns:</p> Name Type Description <code>BasePolicy</code> <p>Greedy policy.</p> Source code in <code>datadynamics/policies/greedy_policy/greedy_policy.py</code> <pre><code>def policy(**kwargs):\n\"\"\"Creates a suitable greedy policy for a given environment.\n    Returns:\n        BasePolicy: Greedy policy.\n    \"\"\"\nif kwargs[\"env\"].metadata[\"name\"] == \"graph_collector\":\npolicy = GraphGreedyPolicy(**kwargs)\nelse:\npolicy = GreedyPolicy(**kwargs)\nreturn policy\n</code></pre>"},{"location":"api/policies/#bfs-greedy-policy","title":"BFS-Greedy Policy","text":""},{"location":"api/policies/#datadynamics.policies.bfs_greedy_policy.bfs_greedy_policy.BFSGraphGreedyPolicy","title":"<code>BFSGraphGreedyPolicy</code>","text":"<p>         Bases: <code>BasePolicy</code></p> <p>Greedy policy using a breadth-first search for every action retrieval.</p> <p>This policy runs in O(V + E) time when finding a new goal for an agent and as such may slow down stepping through the environment.</p> <p>Compatible only with graph_collector environment for graphs with equal edge weights between nodes.</p> <p>Attributes:</p> Name Type Description <code>env</code> <code>pettingzoo.utils.env.AECEnv</code> <p>Environment used by policy.</p> <code>graph</code> <code>nx.Graph</code> <p>Graph used by environment.</p> <code>cur_goals</code> <code>dict</code> <p>Cached goals for each agent consisting of (path, collected, point_idx) tuples keyed by agent name.</p> Source code in <code>datadynamics/policies/bfs_greedy_policy/bfs_greedy_policy.py</code> <pre><code>class BFSGraphGreedyPolicy(BasePolicy):\n\"\"\"Greedy policy using a breadth-first search for every action retrieval.\n    This policy runs in O(V + E) time when finding a new goal for an agent and\n    as such may slow down stepping through the environment.\n    Compatible only with graph_collector environment for graphs with equal\n    edge weights between nodes.\n    Attributes:\n        env (pettingzoo.utils.env.AECEnv): Environment used by policy.\n        graph (nx.Graph): Graph used by environment.\n        cur_goals (dict): Cached goals for each agent consisting of\n            (path, collected, point_idx) tuples keyed by agent name.\n    \"\"\"\ndef __init__(self, env, graph):\n\"\"\"Initialize policy from environment.\n        Args:\n            env (pettingzoo.utils.env.AECEnv): Environment on which to base\n                policy.\n            graph (nx.Graph): Graph used by environment.\n        \"\"\"\nassert env.metadata[\"name\"] == \"graph_collector\", (\nf\"{self.__class__.__name__} is only compatible with \"\n\"graph_collector.\"\n)\nself.env = env\nself.graph = graph\nself.cur_goals = {}\ndef _bfs_shortest_paths(self, source_node, graph):\n\"\"\"Runs breadth-first search to find the shortest paths from a source.\n        Note:\n            This runs in O(V + E) time.\n        Args:\n            source_node (int): Label of source node.\n            graph (nx.Graph): Graph to search.\n        Returns:\n            dict: Dictionary of predecessors and depth keyed by node label.\n        \"\"\"\ndiscovered = {source_node}\npredecessors_and_depth = {source_node: (None, 0)}\nqueue = deque([(source_node, 0)])\nwhile queue:\nnode, depth = queue.popleft()\nfor neighbor in graph.neighbors(node):\nif neighbor not in discovered:\ndiscovered.add(neighbor)\nqueue.append((neighbor, depth + 1))\npredecessors_and_depth[neighbor] = (node, depth)\nreturn predecessors_and_depth\ndef _find_goal_full(self, observation, agent, graph):\n\"\"\"Finds the point with the highest reward for an agent.\n        Args:\n            observation (dict): Observation for agent.\n            agent (str): Name of agent.\n            graph (nx.Graph): Graph used by environment.\n        Returns:\n            tuple: Tuple of (path, collected, point_idx) where path is the\n                shortest path to the point, collected is the number of times\n                the point has been collected, and point_idx is the index of\n                the point in the observation.\n        \"\"\"\nagent_idx = int(agent[-1])\nagent_node = observation[\"collector_labels\"][agent_idx]\npredecessors_and_depth = self._bfs_shortest_paths(agent_node, graph)\nbest_reward = -np.inf\nbest_point_label = None\nbest_point_idx = None\nbest_point_collected = None\nfor i, point_label in enumerate(observation[\"point_labels\"]):\nif point_label not in predecessors_and_depth:\n# Skip unreachable points.\ncontinue\ncollected = observation[\"collected\"][i]\nreward = -predecessors_and_depth[point_label][1]\nif \"collection_reward\" in observation:\nreward += observation[\"collection_reward\"][i]\nif \"cheating_cost\" in observation and collected &gt; 0:\nreward -= observation[\"cheating_cost\"][i]\nif reward &gt; best_reward:\nbest_reward = reward\nbest_point_label = point_label\nbest_point_collected = collected\nbest_point_idx = i\nif best_point_label is None:\nreturn None\n# Backtrack to find the path for the best point.\npath = [best_point_label]\nwhile path[-1] != agent_node and path[-1] is not None:\npath.append(predecessors_and_depth[path[-1]][0])\npath.reverse()\npath = path[1:] + [-1]\nreturn path, best_point_collected, best_point_idx\ndef action(self, observation, agent):\nif self.env.terminations[agent] or self.env.truncations[agent]:\n# Agent is dead, the only valid action is None.\nreturn None\ngoal_path, goal_collected, goal_point_idx = self.cur_goals.get(\nagent, ([], None, None)\n)\nif (\nnot goal_path\nor goal_collected != observation[\"collected\"][goal_point_idx]\n):\ngoal_path, goal_collected, goal_point_idx = self._find_goal_full(\nobservation, agent, self.graph\n)\nself.cur_goals[agent] = (goal_path, goal_collected, goal_point_idx)\naction = goal_path.pop(0) if goal_path else None\nif action is None:\ngymnasium.logger.warn(\nf\"{agent} cannot reach any points and will issue None \"\n\"actions.\"\n)\nreturn action\n</code></pre>"},{"location":"api/policies/#datadynamics.policies.bfs_greedy_policy.bfs_greedy_policy.BFSGraphGreedyPolicy.__init__","title":"<code>__init__(env, graph)</code>","text":"<p>Initialize policy from environment.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>pettingzoo.utils.env.AECEnv</code> <p>Environment on which to base policy.</p> required <code>graph</code> <code>nx.Graph</code> <p>Graph used by environment.</p> required Source code in <code>datadynamics/policies/bfs_greedy_policy/bfs_greedy_policy.py</code> <pre><code>def __init__(self, env, graph):\n\"\"\"Initialize policy from environment.\n    Args:\n        env (pettingzoo.utils.env.AECEnv): Environment on which to base\n            policy.\n        graph (nx.Graph): Graph used by environment.\n    \"\"\"\nassert env.metadata[\"name\"] == \"graph_collector\", (\nf\"{self.__class__.__name__} is only compatible with \"\n\"graph_collector.\"\n)\nself.env = env\nself.graph = graph\nself.cur_goals = {}\n</code></pre>"},{"location":"api/policies/#datadynamics.policies.bfs_greedy_policy.bfs_greedy_policy.policy","title":"<code>policy(**kwargs)</code>","text":"<p>Creates a suitable BFS-based greedy policy for a given environment.</p> <p>Returns:</p> Name Type Description <code>BasePolicy</code> <p>BFS-based greedy policy.</p> Source code in <code>datadynamics/policies/bfs_greedy_policy/bfs_greedy_policy.py</code> <pre><code>def policy(**kwargs):\n\"\"\"Creates a suitable BFS-based greedy policy for a given environment.\n    Returns:\n        BasePolicy: BFS-based greedy policy.\n    \"\"\"\npolicy = BFSGraphGreedyPolicy(**kwargs)\nreturn policy\n</code></pre>"},{"location":"api/policies/#premade-policy","title":"Premade Policy","text":""},{"location":"api/policies/#datadynamics.policies.premade_policy.premade_policy.PremadePolicy","title":"<code>PremadePolicy</code>","text":"<p>         Bases: <code>BasePolicy</code></p> <p>Policy using a premade list of goals for each agent.</p> <p>This policy runs in O(V + E) time when reaching a goal due to having to search for the shortest path to the given goal.</p> <p>Compatible only with graph_collector environment for graphs with equal edge weights between nodes.</p> <p>Attributes:</p> Name Type Description <code>env</code> <code>pettingzoo.utils.env.AECEnv</code> <p>Environment used by policy.</p> <code>graph</code> <code>nx.Graph</code> <p>Graph used by environment.</p> <code>cur_goals</code> <code>dict</code> <p>Cached goals for each agent consisting of (path, collected, point_idx) tuples keyed by agent name.</p> Source code in <code>datadynamics/policies/premade_policy/premade_policy.py</code> <pre><code>class PremadePolicy(BasePolicy):\n\"\"\"Policy using a premade list of goals for each agent.\n    This policy runs in O(V + E) time when reaching a goal due to having to\n    search for the shortest path to the given goal.\n    Compatible only with graph_collector environment for graphs with equal\n    edge weights between nodes.\n    Attributes:\n        env (pettingzoo.utils.env.AECEnv): Environment used by policy.\n        graph (nx.Graph): Graph used by environment.\n        cur_goals (dict): Cached goals for each agent consisting of\n            (path, collected, point_idx) tuples keyed by agent name.\n    \"\"\"\ndef __init__(self, env, graph, goal_dict):\n\"\"\"Initialize policy from environment.\n        Args:\n            env (pettingzoo.utils.env.AECEnv): Environment on which to base\n                policy.\n            graph (nx.Graph): Graph used by environment.\n            goal_dict (dict): Dictionary of goals for each agent (keys can be\n                arbitrary).\n        \"\"\"\nassert env.metadata[\"name\"] == \"graph_collector\", (\nf\"{self.__class__.__name__} is only compatible with \"\n\"graph_collector.\"\n)\nself.env = env\nself.graph = graph\nself.cur_goals = {}\nself.goal_dict = {}\nassert len(goal_dict) == len(env.possible_agents), (\nf\"You must provide only one list of goals for each agent. \"\nf\"Provided {len(goal_dict)} goals for {len(env.possible_agents)} \"\n\"agents.\"\n)\nfor agent, key in zip(env.possible_agents, goal_dict):\n# Copy goals since we pop from them.\nself.goal_dict[agent] = goal_dict[key][:]\ndef _bfs_shortest_path(self, source_node, target, graph):\n\"\"\"Runs BFS to find the shortest path from source to target.\n        Note:\n            This runs in O(V + E) time in the worst case.\n        Args:\n            source_node (int): Label of source node.\n            target (int): Label of target node.\n            graph (nx.Graph): Graph to search.\n        Returns:\n            list: Shortest path from source to target\n        Raises:\n            ValueError: If no path exists from source to target.\n        \"\"\"\ndiscovered = {source_node}\npredecessors_and_depth = {source_node: (None, 0)}\nqueue = deque([(source_node, 0)])\nwhile queue:\nnode, depth = queue.popleft()\nif node == target:\n# Reconstruct path\npath = [target]\nwhile path[-1] != source_node and path[-1] is not None:\npath.append(predecessors_and_depth[path[-1]][0])\npath.reverse()\npath = path[1:] + [-1]\nreturn path\nfor neighbor in graph.neighbors(node):\nif neighbor not in discovered:\ndiscovered.add(neighbor)\nqueue.append((neighbor, depth + 1))\npredecessors_and_depth[neighbor] = (node, depth)\nraise ValueError(f\"No path exists from {source_node} to {target}.\")\ndef action(self, observation, agent):\nif self.env.terminations[agent] or self.env.truncations[agent]:\n# Agent is dead, the only valid action is None.\nif self.goal_dict[agent]:\ngymnasium.logger.warn(\nf\"Agent {agent} is dead but has goals remaining.\"\n)\nreturn None\ngoal_path = self.cur_goals.get(agent, ([]))\nif not goal_path:\nagent_idx = int(agent[-1])\nagent_node = observation[\"collector_labels\"][agent_idx]\ngoal_path = self._bfs_shortest_path(\nagent_node, self.goal_dict[agent].pop(0), self.graph\n)\nself.cur_goals[agent] = goal_path\naction = goal_path.pop(0)\nreturn action\n</code></pre>"},{"location":"api/policies/#datadynamics.policies.premade_policy.premade_policy.PremadePolicy.__init__","title":"<code>__init__(env, graph, goal_dict)</code>","text":"<p>Initialize policy from environment.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>pettingzoo.utils.env.AECEnv</code> <p>Environment on which to base policy.</p> required <code>graph</code> <code>nx.Graph</code> <p>Graph used by environment.</p> required <code>goal_dict</code> <code>dict</code> <p>Dictionary of goals for each agent (keys can be arbitrary).</p> required Source code in <code>datadynamics/policies/premade_policy/premade_policy.py</code> <pre><code>def __init__(self, env, graph, goal_dict):\n\"\"\"Initialize policy from environment.\n    Args:\n        env (pettingzoo.utils.env.AECEnv): Environment on which to base\n            policy.\n        graph (nx.Graph): Graph used by environment.\n        goal_dict (dict): Dictionary of goals for each agent (keys can be\n            arbitrary).\n    \"\"\"\nassert env.metadata[\"name\"] == \"graph_collector\", (\nf\"{self.__class__.__name__} is only compatible with \"\n\"graph_collector.\"\n)\nself.env = env\nself.graph = graph\nself.cur_goals = {}\nself.goal_dict = {}\nassert len(goal_dict) == len(env.possible_agents), (\nf\"You must provide only one list of goals for each agent. \"\nf\"Provided {len(goal_dict)} goals for {len(env.possible_agents)} \"\n\"agents.\"\n)\nfor agent, key in zip(env.possible_agents, goal_dict):\n# Copy goals since we pop from them.\nself.goal_dict[agent] = goal_dict[key][:]\n</code></pre>"},{"location":"api/policies/#datadynamics.policies.premade_policy.premade_policy.policy","title":"<code>policy(**kwargs)</code>","text":"<p>Creates a premade policy for a given environment</p> <p>Returns:</p> Name Type Description <code>BasePolicy</code> <p>Premade policy.</p> Source code in <code>datadynamics/policies/premade_policy/premade_policy.py</code> <pre><code>def policy(**kwargs):\n\"\"\"Creates a premade policy for a given environment\n    Returns:\n        BasePolicy: Premade policy.\n    \"\"\"\npolicy = PremadePolicy(**kwargs)\nreturn policy\n</code></pre>"},{"location":"api/policies/#random-policy","title":"Random Policy","text":""},{"location":"api/policies/#datadynamics.policies.random_policy.random_policy.RandomPolicy","title":"<code>RandomPolicy</code>","text":"<p>         Bases: <code>BasePolicy</code></p> <p>Policy that returns a random action.</p> <p>Compatible with all environments.</p> <p>Attributes:</p> Name Type Description <code>env</code> <code>pettingzoo.utils.env.AECEnv</code> <p>Environment used by policy.</p> Source code in <code>datadynamics/policies/random_policy/random_policy.py</code> <pre><code>class RandomPolicy(BasePolicy):\n\"\"\"Policy that returns a random action.\n    Compatible with all environments.\n    Attributes:\n        env (pettingzoo.utils.env.AECEnv): Environment used by policy.\n    \"\"\"\ndef __init__(self, env):\nself.env = env\ndef action(self, observation, agent):\nif self.env.terminations[agent] or self.env.truncations[agent]:\n# Agent is dead, the only valid action is None.\nreturn None\naction = self.env.action_space(agent).sample()\nreturn action\n</code></pre>"},{"location":"api/policies/#datadynamics.policies.random_policy.random_policy.policy","title":"<code>policy(**kwargs)</code>","text":"<p>Creates a RandomPolicy for a given environment.</p> <p>Returns:</p> Name Type Description <code>BasePolicy</code> <p>Random policy.</p> Source code in <code>datadynamics/policies/random_policy/random_policy.py</code> <pre><code>def policy(**kwargs):\n\"\"\"Creates a RandomPolicy for a given environment.\n    Returns:\n        BasePolicy: Random policy.\n    \"\"\"\npolicy = RandomPolicy(**kwargs)\nreturn policy\n</code></pre>"},{"location":"api/policies/#dummy-policy","title":"Dummy Policy","text":""},{"location":"api/policies/#datadynamics.policies.dummy_policy.dummy_policy.DummyPolicy","title":"<code>DummyPolicy</code>","text":"<p>         Bases: <code>BasePolicy</code></p> <p>Dummy policy that cycles through all actions.</p> <p>Compatible with all environments.</p> <p>Attributes:</p> Name Type Description <code>env</code> <code>pettingzoo.utils.env.AECEnv</code> <p>Environment used by policy.</p> Source code in <code>datadynamics/policies/dummy_policy/dummy_policy.py</code> <pre><code>class DummyPolicy(BasePolicy):\n\"\"\"Dummy policy that cycles through all actions.\n    Compatible with all environments.\n    Attributes:\n        env (pettingzoo.utils.env.AECEnv): Environment used by policy.\n    \"\"\"\ndef __init__(self, env):\nself.env = env\nself._actions = self._get_possible_actions(env)\ndef _get_possible_actions(self, env):\n\"\"\"Retrieve all possible actions for given environment\n        Args:\n            env (pettingzoo.utils.env.AECEnv): Environment for which to\n                retrieve actions.\n        Returns:\n            dict: Dictionary of action iterators keyed by agent.\n        \"\"\"\nactions = {}\nfor agent in env.possible_agents:\naction_space = env.action_spaces[agent]\nactions[agent] = cycle(\nrange(action_space.start, action_space.n + action_space.start)\n)\nreturn actions\ndef action(self, observation, agent):\nif self.env.terminations[agent] or self.env.truncations[agent]:\n# Agent is dead, the only valid action is None.\nreturn None\naction = next(self._actions[agent])\nreturn action\n</code></pre>"},{"location":"api/policies/#datadynamics.policies.dummy_policy.dummy_policy.policy","title":"<code>policy(**kwargs)</code>","text":"<p>Creates a dummy policy for a given environment.</p> <p>Returns:</p> Name Type Description <code>BasePolicy</code> <p>Dummy policy.</p> Source code in <code>datadynamics/policies/dummy_policy/dummy_policy.py</code> <pre><code>def policy(**kwargs):\n\"\"\"Creates a dummy policy for a given environment.\n    Returns:\n        BasePolicy: Dummy policy.\n    \"\"\"\npolicy = DummyPolicy(**kwargs)\nreturn policy\n</code></pre>"},{"location":"api/post_processing/","title":"Post Processing","text":""},{"location":"api/post_processing/#extracting-simulation-data","title":"Extracting Simulation Data","text":""},{"location":"api/post_processing/#datadynamics.utils.post_processing.extract.collections","title":"<code>collections(env)</code>","text":"<p>Returns a dict of point labels collected by each agent with timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>AECEnv</code> <p>Env to extract point labels from.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Dict of point labels together with timestamps keyed by agent name.</p> Source code in <code>datadynamics/utils/post_processing/extract.py</code> <pre><code>def collections(env):\n\"\"\"Returns a dict of point labels collected by each agent with timestamps.\n    Args:\n        env (AECEnv): Env to extract point labels from.\n    Returns:\n        dict: Dict of point labels together with timestamps keyed by agent\n            name.\n    \"\"\"\ncolls = {}\nfor agent in env.possible_agents:\ncollector = env.collectors[agent]\npoints_and_timestamps = [\n[point.label, timestamp] for point, timestamp in collector.points\n]\ncolls[agent] = points_and_timestamps\nreturn colls\n</code></pre>"},{"location":"api/post_processing/#datadynamics.utils.post_processing.extract.feats_targets_timestamps","title":"<code>feats_targets_timestamps(collections, include_timestamps=True)</code>","text":"<p>Extracts features, targets and timestamps from collections.</p> <p>Parameters:</p> Name Type Description Default <code>collections</code> <code>dict</code> <p>Dict of point labels together with timestamps keyed by agent name.</p> required <code>include_timestamps</code> <code>bool</code> <p>Whether to include timestamps in features. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>Tuple of timestamps, features and targets.</p> Source code in <code>datadynamics/utils/post_processing/extract.py</code> <pre><code>def feats_targets_timestamps(collections, include_timestamps=True):\n\"\"\"Extracts features, targets and timestamps from collections.\n    Args:\n        collections (dict): Dict of point labels together with timestamps keyed\n            by agent name.\n        include_timestamps (bool, optional): Whether to include timestamps in\n            features. Defaults to True.\n    Returns:\n        tuple: Tuple of timestamps, features and targets.\n    \"\"\"\ntimestamps = []\nfeats = []\ntargets = []\nfor i, feat in enumerate(collections.values()):\ntimestamps += [f[-1] for f in feat]\nif not include_timestamps:\nfeat = [f[:-1] for f in feat]\nfeats += feat\ntargets += [i] * len(feat)\nsort_indices = np.argsort(timestamps)\nfeats = np.array(feats)[sort_indices]\ntargets = np.array(targets)[sort_indices]\ntimestamps = np.array(timestamps)[sort_indices]\nreturn timestamps, feats, targets\n</code></pre>"},{"location":"api/post_processing/#datadynamics.utils.post_processing.extract.save_collections","title":"<code>save_collections(env, filename)</code>","text":"<p>Extracts and saves point labels collected by each agent with timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>env</code> <code>AECEnv</code> <p>Env to extract point labels from.</p> required <code>filename</code> <code>str</code> <p>Filename to save point labels to.</p> required Source code in <code>datadynamics/utils/post_processing/extract.py</code> <pre><code>def save_collections(env, filename):\n\"\"\"Extracts and saves point labels collected by each agent with timestamps.\n    Args:\n        env (AECEnv): Env to extract point labels from.\n        filename (str): Filename to save point labels to.\n    \"\"\"\ncolls = collections(env)\nwith open(filename, \"wb\") as f:\npickle.dump(colls, f)\nprint(f\"Saved collections to {filename}.\")\n</code></pre>"},{"location":"api/scripts/","title":"Scripts","text":""},{"location":"api/scripts/#graphs","title":"Graphs","text":"<p>Datadynamics contains several scripts to generate graphs and point labels from image files to use in the <code>graph_collector</code> environment.</p>"},{"location":"api/scripts/#generate-graphs-from-masks","title":"Generate Graphs From Masks","text":"<p>Script to generate input graphs to the graph_collector_v0 environment.</p> <p>This script generates a graph  with a grid-like structure from an obstacle mask file. Obstacles are encoding by black as default. The input obstacle mask file should be a binary image or an image that can be converted to one. The file should be a mask that represents the obstacles in the grid-like environment that the agent cannot traverse through.</p>"},{"location":"api/scripts/#scripts.graph.generate_graph.main","title":"<code>main(args)</code>","text":"<p>Generate graph from obstacle mask file.</p> <p>Example usage: python -m datadynamics.scripts.graph.generate_graph         -i ./data/obstacle_mask.png         -o ./data/graph.pkl         -m ./data/metadata.json         -rs 100 100         -dfw 1.0</p> Source code in <code>scripts/graph/generate_graph.py</code> <pre><code>def main(args):\n\"\"\"Generate graph from obstacle mask file.\n    Example usage:\n    python -m datadynamics.scripts.graph.generate_graph \\\n        -i ./data/obstacle_mask.png \\\n        -o ./data/graph.pkl \\\n        -m ./data/metadata.json \\\n        -rs 100 100 \\\n        -dfw 1.0\n    \"\"\"\ngraph, metadata = graph_extractor.from_mask_file(\nargs.input_file,\nresize=args.resize,\ndefault_weight=args.default_weight,\ndefault_self_loop_weight=args.default_self_loop_weight,\ninverted=args.inverted,\nflip=args.flip,\n)\nwith open(args.output_file, \"wb\") as f:\npickle.dump(graph, f)\nprint(f\"Saved graph to {args.output_file}.\")\nwith open(args.metadata, \"w\") as f:\njson.dump(metadata, f, indent=4)\nprint(f\"Saved metadata to {args.metadata}.\")\n</code></pre>"},{"location":"api/scripts/#generate-point-labels-from-masks","title":"Generate Point Labels From Masks","text":"<p>Script to generate point labels for the graph_collector_v0 environment.</p> <p>This script generates point labels used to represent points in a grid-like environment structure from a point mask file. Points are encoding by black as default. The input point mask file should be a binary image or an image that can be converted to one.</p>"},{"location":"api/scripts/#scripts.graph.generate_points.main","title":"<code>main(args)</code>","text":"<p>Generate point labels from point mask file.</p> <p>Example usage: python -m datadynamics.scripts.graph.generate_points         -i ./data/point_mask.png         -o ./data/point_labels.pkl         -rs 100 100</p> Source code in <code>scripts/graph/generate_points.py</code> <pre><code>def main(args):\n\"\"\"Generate point labels from point mask file.\n    Example usage:\n    python -m datadynamics.scripts.graph.generate_points \\\n        -i ./data/point_mask.png \\\n        -o ./data/point_labels.pkl \\\n        -rs 100 100\n    \"\"\"\npoint_labels = point_extractor.from_mask_file(\nargs.input_file,\nresize=args.resize,\ninverted=args.inverted,\nflip=args.flip,\n)\nwith open(args.output_file, \"wb\") as f:\npickle.dump(point_labels, f)\nprint(f\"Saved point_labels to {args.output_file}.\")\n</code></pre>"},{"location":"tutorials/collector_tutorial/","title":"Simulating Data Collection Dynamics in a Plane","text":"<p>In this tutorial, we will explore how to use Datadynamics to simulate data collection tasks in a plane-based environment.</p>"},{"location":"tutorials/collector_tutorial/#plane-based-environments","title":"Plane-Based Environments","text":"<p>The plane-based environment provides a simple space for quick iterations and testing of new strategies. The environment is based on a two-dimensional Euclidean space (using (x, y) coordinates), where agents can navigate the environment with a cost proportional to the Euclidean distance travelled. Although this environment does not allow for complex terrain and obstacle representations, it is optimized for performance.</p>"},{"location":"tutorials/collector_tutorial/#example-of-plane-based-collector-environment","title":"Example of Plane-Based Collector Environment","text":"<p>In the following example, we will define a simple a Euclidean plane consisting of 300 points sampled uniformly at random in the range [0, 10] for each dimension. We also define two agents that start at coordinates (0, 0) and (1, 1), respectively. The agents can collect a maximum of 120 and 180 points. The behavior of the agents are specified by a dummy policy that simply cycles through the available actions (i.e. collecting a point) in a round-robin fashion.</p> <pre><code>import numpy as np\nfrom datadynamics.environments import collector_v0\nfrom datadynamics.policies import dummy_policy_v0\nenv = collector_v0.env(\npoint_positions=np.random.uniform(0, 10, (300, 2)),\ninit_agent_positions=np.array([[0, 0], [1, 1]]),\nmax_collect=[120, 180],\nrender_mode=\"human\",\n)\npolicy = dummy_policy_v0.policy(env=env)\nenv.reset()\nfor agent in env.agent_iter():\nobservation, reward, termination, truncation, info = env.last()\naction = policy.action(observation, agent)\nenv.step(action)\n</code></pre>"},{"location":"tutorials/collector_tutorial/#example-of-wrapper-environment","title":"Example of Wrapper Environment","text":"<p>Datadynamics also provides a wrapper environment that can be used to randomly generate points based on a given sampler and a given number of points. In the following example, we will again define a simple Euclidean plane consisting of 300 points, but this time sampled randomly from a standard Normal. Agents can again collect a maximum of 120 and 180 points. The behavior of the agents are, however, instead specified by a greedy policy that always collects the point with the highest expected reward.</p> <pre><code>from datadynamics.environments import collector_v0\nfrom datadynamics.policies import greedy_policy_v0\nenv = collector_v0.env(\nn_points=300, n_agents=2, max_collect=[120, 180], render_mode=\"human\"\n)\npolicy = greedy_policy_v0.policy(env=env)\nenv.reset()\nfor agent in env.agent_iter():\nobservation, reward, termination, truncation, info = env.last()\naction = policy.action(observation, agent)\nenv.step(action)\n</code></pre>"},{"location":"tutorials/graph_collector_tutorial/","title":"Simulating Data Collection Dynamics with a Graph","text":"<p>In this tutorial, we will explore how to use Datadynamics to simulate data collection tasks in a graph-based environment.</p>"},{"location":"tutorials/graph_collector_tutorial/#graph-based-environments","title":"Graph-based Environments","text":"<p>The graph-based environment encodes its structure as a weighted and possibly directed graph, supporting both an underlying dynamic and changing graph as well as static graphs. Agents can traverse the graph to collect data points for rewards, where the cost of traversing is determined by the edge weights. The graph encoding enables the creation of almost all environment structures, including the approximation of the plane-based environment through a grid or navigation mesh-like structure. However, this comes at a performance cost due to the increased number of objects to render and the often high time complexity of graph algorithms used for pathfinding in some policies.</p>"},{"location":"tutorials/graph_collector_tutorial/#defining-a-graph","title":"Defining a Graph","text":"<p>In Datadynamics, graphs should be encoded as a weighted (directed or undirected) graph using the NetworkX library. Each node in the graph represents a location that the agents can occupy, and each edge represents a connection between two locations. The weight of an edge represents the cost (negated reward) of traversing it.</p> <p>The graphs should also contain node that represent points that agents can collect.</p> <p>Graphs can be created in a variety of different ways as specified in the NetworkX documentation. A simple way to create a graph is from an adjacency matrix indicated whether pairs of nodes are adjacent or not in the graph. You can e.g. use the following code:</p> <pre><code>import networkx as nx\nimport numpy as np\nadjacency_matrix = np.array(\n[\n[1, 1, 1, 0],\n[1, 1, 0, 1],\n[1, 0, 1, 1],\n[0, 1, 1, 1],\n]\n)\ngraph = nx.from_numpy_array(adjacency_matrix)\n</code></pre> <p>In this example, we define an undirected graph with four nodes that are all locally connected. Each edge has a weight of 1.</p>"},{"location":"tutorials/graph_collector_tutorial/#example-of-graph-collector-environment","title":"Example of Graph-collector Environment","text":"<p>In the following example, we will define a simple graph with nine nodes that are all locally connected. Three of the nodes define collectable points that the two agents aim to collect. The behavior of the agents are set by a greedy policy that always collects the point with the highest expected reward in a round-robin fashion. Each agent can collect a maximum of 3 points and start out at nodes 0 and 1, respectively. The environment API follows the standard set out by the PettingZoo library.</p> <pre><code>import networkx as nx\nimport numpy as np\nfrom datadynamics.environments import graph_collector_v0\nfrom datadynamics.policies import greedy_policy_v0\n\"\"\"\nDefine following graph:\nA A P\nP N N\nN - P\nwhere\nA = agents,\nN = nodes,\nP = collectable points,\n- = obstacles.\n\"\"\"\nadjacency_matrix = np.array(\n[\n[1, 1, 0, 1, 0, 0, 0, 0, 0],\n[1, 1, 1, 0, 1, 0, 0, 0, 0],\n[0, 1, 1, 0, 0, 1, 0, 0, 0],\n[1, 0, 0, 1, 1, 0, 1, 0, 0],\n[0, 1, 0, 1, 1, 1, 0, 0, 0],\n[0, 0, 1, 0, 1, 1, 0, 0, 1],\n[0, 0, 0, 1, 0, 0, 1, 0, 0],\n[0, 0, 0, 0, 0, 0, 0, 0, 0],\n[0, 0, 0, 0, 0, 1, 0, 0, 1],\n]\n)\ngraph = nx.from_numpy_array(adjacency_matrix)\npoints_labels = [2, 3, 8]\ninit_agent_labels = [0, 1]\nmax_colllect = [3, 3]\nenv = graph_collector_v0.env(\ngraph=graph,\npoint_labels=points_labels,\ninit_agent_labels=init_agent_labels,\nmax_collect=max_colllect,\nrender_mode=\"human\",\n)\npolicy = greedy_policy_v0.policy(env=env)\nenv.reset()\nfor agent in env.agent_iter():\nobservation, reward, termination, truncation, info = env.last()\naction = policy.action(observation, agent)\nenv.step(action)\n</code></pre>"}]}